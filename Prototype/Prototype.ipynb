{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzVGbQcQTDPx",
        "outputId": "da705a07-16c3-4436-935b-2f6412903a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "RUNNING UPDATED SKLEARN DATA PIPELINE\n",
            "==================================================\n",
            "Loading data...\n",
            "Loaded 100,000 rows\n",
            "Successfully loaded 100,000 rows\n",
            "Applying preprocessing steps...\n",
            "Applying column_dropper...\n",
            "Applying address_processor...\n",
            "Applying email_validator...\n",
            "Applying boolean_converter...\n",
            "Applying income_encoder...\n",
            "Applying date_extractor...\n",
            "Processed data saved to processed_data.csv\n",
            "Pipeline saved as 'data_pipeline.pkl'\n",
            "Pipeline completed successfully!\n",
            "Original shape: (100000, 110)\n",
            "Processed shape: (100000, 179)\n",
            "\n",
            "Pipeline Summary:\n",
            "Processed data shape: (100000, 179)\n",
            "Processed data columns: 179\n",
            "\n",
            "Sample of processed data:\n",
            "    Customer_id  Loan_Account_id  Loan_Amount_SGD  Outstanding_Balance_SGD  \\\n",
            "0  SCB843421788         32765033          10000.0                  8591.56   \n",
            "1  SCB998027725         87850971          70000.0                 51361.65   \n",
            "2  SCB871158951         24716289          52000.0                 44262.73   \n",
            "3  SCB938686930         36505419         125000.0                125000.00   \n",
            "4  SCB843983697         73869477          40000.0                 37241.07   \n",
            "\n",
            "   Day_Past_Due  Tenure  Interest_Rate  Current_EMI_SGD  \\\n",
            "0             0      60           10.0           212.47   \n",
            "1            12      24            9.5          3278.29   \n",
            "2            43      12            8.0          4885.27   \n",
            "3             0      36            9.5          4004.12   \n",
            "4             0      48           10.0          1014.50   \n",
            "\n",
            "   Partial_Payment_Indicator  Number_of_Past_Payments  ...  \\\n",
            "0                      False                       12  ...   \n",
            "1                       True                        8  ...   \n",
            "2                      False                        4  ...   \n",
            "3                       True                        1  ...   \n",
            "4                       True                        4  ...   \n",
            "\n",
            "   AAR_Risk_Level_Low  AAR_Risk_Level_Medium  Region_Battery Road  \\\n",
            "0                 1.0                    0.0                  0.0   \n",
            "1                 1.0                    0.0                  1.0   \n",
            "2                 1.0                    0.0                  0.0   \n",
            "3                 1.0                    0.0                  0.0   \n",
            "4                 1.0                    0.0                  1.0   \n",
            "\n",
            "   Region_Bukit Timah  Region_Harbourfront  Region_Marina Bay  \\\n",
            "0                 0.0                  1.0                0.0   \n",
            "1                 0.0                  0.0                0.0   \n",
            "2                 0.0                  1.0                0.0   \n",
            "3                 0.0                  0.0                0.0   \n",
            "4                 0.0                  0.0                0.0   \n",
            "\n",
            "   Region_Marine Parade  Region_Orchard  Region_Serangoon Garden  \\\n",
            "0                   0.0             0.0                      0.0   \n",
            "1                   0.0             0.0                      0.0   \n",
            "2                   0.0             0.0                      0.0   \n",
            "3                   0.0             0.0                      1.0   \n",
            "4                   0.0             0.0                      0.0   \n",
            "\n",
            "   Region_Suntec City  \n",
            "0                 0.0  \n",
            "1                 0.0  \n",
            "2                 0.0  \n",
            "3                 0.0  \n",
            "4                 0.0  \n",
            "\n",
            "[5 rows x 179 columns]\n",
            "\n",
            "Data types after processing:\n",
            "float64    116\n",
            "int64       33\n",
            "bool        13\n",
            "object      11\n",
            "int32        6\n",
            "Name: count, dtype: int64\n",
            "\n",
            "New columns created by data cleaning:\n",
            "Area_From_Address: 100000 non-null values\n",
            "Pincode_From_Address: 100000 non-null values\n",
            "Email_Valid_Format: 100000 non-null values\n",
            "Email_Domain: 100000 non-null values\n",
            "Email_Domain_Legitimate: 100000 non-null values\n",
            "Email_Disposable: 100000 non-null values\n",
            "\n",
            "Date features created:\n",
            "Installment_Due_Date_month: 100000 non-null values\n",
            "Installment_Due_Date_dayofweek: 100000 non-null values\n",
            "Installment_Due_Date_year: 100000 non-null values\n",
            "Last_Payment_Date_month: 100000 non-null values\n",
            "Last_Payment_Date_dayofweek: 100000 non-null values\n",
            "Last_Payment_Date_year: 100000 non-null values\n",
            "\n",
            "Agent columns preserved as-is:\n",
            "Last_Successful_Agent_ID: 100000 non-null values\n",
            "Sample values: ['SCB_AG_0025', 'SCB_AG_0174', 'SCB_AG_0051']\n",
            "Best_Contact_Agent_IDs: 100000 non-null values\n",
            "Sample values: ['SCB_AG_0025', 'SCB_AG_0174', 'SCB_AG_0051']\n",
            "\n",
            "Region encoded columns (8):\n",
            "  Region_Battery Road: 12506.0 records\n",
            "  Region_Bukit Timah: 12505.0 records\n",
            "  Region_Harbourfront: 12549.0 records\n",
            "  Region_Marina Bay: 12573.0 records\n",
            "  Region_Marine Parade: 12758.0 records\n",
            "  ... and 3 more region columns\n",
            "\n",
            "Area_From_Address (preserved as-is):\n",
            "  Unique values: 36\n",
            "  Sample values: ['Harbourfront', 'Market Street', 'Harbourfront', 'Serangoon Garden', 'Raffles Place']\n",
            "\n",
            "Pincode_From_Address (preserved as-is):\n",
            "  Unique values: 24\n",
            "  Sample values: ['098585', '048940', '098585', '558057', '048621']\n",
            "\n",
            "Phone number columns removed:\n",
            "  âœ“ Primary_Phone_Number successfully removed\n",
            "  âœ“ Secondary_Mobile_Number successfully removed\n",
            "  âœ“ Landline_Phone_Number successfully removed\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import joblib\n",
        "import warnings\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Custom transformers for specific data processing tasks\n",
        "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract features from date columns\"\"\"\n",
        "    def __init__(self, date_columns):\n",
        "        self.date_columns = date_columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for col in self.date_columns:\n",
        "            if col in X.columns:\n",
        "                X[col] = pd.to_datetime(X[col], errors='coerce')\n",
        "                X[f'{col}_month'] = X[col].dt.month\n",
        "                X[f'{col}_dayofweek'] = X[col].dt.dayofweek\n",
        "                X[f'{col}_year'] = X[col].dt.year\n",
        "                # Drop original date column\n",
        "                X = X.drop(columns=[col])\n",
        "        return X\n",
        "\n",
        "class IncomeBandEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Encode income bands to numerical values\"\"\"\n",
        "    def __init__(self):\n",
        "        self.income_mapping = {\n",
        "            '50,000 or Below': 0,\n",
        "            '50,000 to 100,000': 1,\n",
        "            '100,000 to 200,000': 2,\n",
        "            '200,000 to 300,000': 3,\n",
        "            '300,000 to 500,000': 4,\n",
        "            '500,000 or Above': 5\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if 'Income_Band_SGD' in X.columns:\n",
        "            X['Income_Band_SGD'] = X['Income_Band_SGD'].map(self.income_mapping)\n",
        "        return X\n",
        "\n",
        "class BooleanConverter(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Convert various boolean representations to proper booleans\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        bool_columns = ['Partial_Payment_Indicator', 'Repayment_Irregularity_Flags',\n",
        "                       'Mobile_Number_Active_Status', 'Email_Activity', 'Do_Not_Call_Registry_Data',\n",
        "                       'WhatsApp_OTT_usage_Indicator', 'Overdraft_or_Low_Balance_Flag',\n",
        "                       'Delinquency_on_other_Loans']\n",
        "\n",
        "        for col in bool_columns:\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].astype(str).str.lower().replace({\n",
        "                    'true': True, 'false': False, '1': True, '0': False,\n",
        "                    'yes': True, 'no': False\n",
        "                }).astype(bool)\n",
        "        return X\n",
        "\n",
        "class AddressProcessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract area and pincode from address column but don't encode them\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if 'Address' in X.columns:\n",
        "            # Extract area and pincode from address (these will be kept as passthrough, not encoded)\n",
        "            extracted_data = X['Address'].apply(self._extract_area_and_pincode)\n",
        "            X['Area_From_Address'] = extracted_data.apply(lambda x: x[0])\n",
        "            X['Pincode_From_Address'] = extracted_data.apply(lambda x: x[1])\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _extract_area_and_pincode(self, address):\n",
        "        \"\"\"Extract area name and pincode from address\"\"\"\n",
        "        if pd.isna(address):\n",
        "            return (None, None)\n",
        "\n",
        "        # Common Singapore areas (from your data generation code)\n",
        "        singapore_areas = [\n",
        "            \"Raffles Place\", \"Marina Bay\", \"Suntec City\", \"Harbourfront\",\n",
        "            \"Serangoon Garden\", \"Marine Parade\", \"Bukit Timah\", \"Orchard\",\n",
        "            \"Tanjong Pagar\", \"Chinatown\", \"Little India\", \"Kampong Glam\",\n",
        "            \"Bugis\", \"Dhoby Ghaut\", \"Somerset\", \"City Hall\",\n",
        "            \"Lavender\", \"Kallang\", \"Geylang\", \"Eunos\",\n",
        "            \"Bedok\", \"Tampines\", \"Pasir Ris\", \"Simei\",\n",
        "            \"Jurong East\", \"Jurong West\", \"Clementi\", \"Bukit Batok\",\n",
        "            \"Bukit Panjang\", \"Choa Chu Kang\", \"Woodlands\", \"Yishun\",\n",
        "            \"Sembawang\", \"Ang Mo Kio\", \"Bishan\", \"Toa Payoh\",\n",
        "            \"Serangoon\", \"Hougang\", \"Punggol\", \"Sengkang\"\n",
        "        ]\n",
        "\n",
        "        # Look for area names in the address\n",
        "        address_lower = address.lower()\n",
        "        area_found = None\n",
        "\n",
        "        for area in singapore_areas:\n",
        "            if area.lower() in address_lower:\n",
        "                area_found = area\n",
        "                break\n",
        "\n",
        "        # Extract pincode (6-digit Singapore pincode)\n",
        "        pincode_match = re.search(r'(\\d{6})', address)\n",
        "        pincode = pincode_match.group(1) if pincode_match else None\n",
        "\n",
        "        # If no area found using exact match, try to extract from address structure\n",
        "        if area_found is None:\n",
        "            # Try to extract the word before \"Singapore\" or look for common patterns\n",
        "            match = re.search(r',\\s*([^,]+?),\\s*Singapore', address, re.IGNORECASE)\n",
        "            if match:\n",
        "                potential_area = match.group(1).strip()\n",
        "                # Check if this potential area is in our list\n",
        "                for area in singapore_areas:\n",
        "                    if area.lower() in potential_area.lower():\n",
        "                        area_found = area\n",
        "                        break\n",
        "                else:\n",
        "                    area_found = potential_area  # Use as-is if not in list\n",
        "\n",
        "        return (area_found, pincode)\n",
        "\n",
        "class EmailValidator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Validate email addresses and extract domain information\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        if 'Email_ID' in X.columns:\n",
        "            # Validate email format\n",
        "            X['Email_Valid_Format'] = X['Email_ID'].apply(self._validate_email_format)\n",
        "\n",
        "            # Extract domain\n",
        "            X['Email_Domain'] = X['Email_ID'].apply(self._extract_domain)\n",
        "\n",
        "            # Check if domain is legitimate (common email providers)\n",
        "            X['Email_Domain_Legitimate'] = X['Email_Domain'].apply(self._is_legitimate_domain)\n",
        "\n",
        "            # Check for disposable email domains\n",
        "            X['Email_Disposable'] = X['Email_Domain'].apply(self._is_disposable_domain)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _validate_email_format(self, email):\n",
        "        \"\"\"Validate basic email format\"\"\"\n",
        "        if pd.isna(email) or not isinstance(email, str):\n",
        "            return False\n",
        "\n",
        "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
        "        return bool(re.match(email_pattern, email))\n",
        "\n",
        "    def _extract_domain(self, email):\n",
        "        \"\"\"Extract domain from email address\"\"\"\n",
        "        if pd.isna(email) or not isinstance(email, str) or '@' not in email:\n",
        "            return None\n",
        "\n",
        "        return email.split('@')[1].lower()\n",
        "\n",
        "    def _is_legitimate_domain(self, domain):\n",
        "        \"\"\"Check if domain is from a legitimate email provider\"\"\"\n",
        "        if pd.isna(domain):\n",
        "            return False\n",
        "\n",
        "        legitimate_domains = [\n",
        "            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'live.com',\n",
        "            'icloud.com', 'protonmail.com', 'zoho.com', 'aol.com', 'mail.com',\n",
        "            'gmail.com.sg', 'yahoo.com.sg', 'hotmail.com.sg', 'singnet.com.sg',\n",
        "            'starhub.net.sg', 'pacific.net.sg'\n",
        "        ]\n",
        "\n",
        "        return domain in legitimate_domains\n",
        "\n",
        "    def _is_disposable_domain(self, domain):\n",
        "        \"\"\"Check if domain is from a disposable email service\"\"\"\n",
        "        if pd.isna(domain):\n",
        "            return False\n",
        "\n",
        "        disposable_domains = [\n",
        "            'tempmail.com', '10minutemail.com', 'guerrillamail.com',\n",
        "            'mailinator.com', 'yopmail.com', 'trashmail.com',\n",
        "            'disposableemail.com', 'fakeinbox.com', 'temp-mail.org'\n",
        "        ]\n",
        "\n",
        "        return domain in disposable_domains\n",
        "\n",
        "class DataLoader(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Load and initial data processing\"\"\"\n",
        "    def __init__(self, file_path, chunk_size=10000):\n",
        "        self.file_path = file_path\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X=None):\n",
        "        \"\"\"Load data from CSV file\"\"\"\n",
        "        print(\"Loading data...\")\n",
        "\n",
        "        try:\n",
        "            # Load data in chunks for memory efficiency\n",
        "            chunks = []\n",
        "            for i, chunk in enumerate(pd.read_csv(self.file_path, chunksize=self.chunk_size)):\n",
        "                chunks.append(chunk)\n",
        "                if (i + 1) % 10 == 0:  # Print progress every 10 chunks\n",
        "                    print(f\"Loaded {min((i+1)*self.chunk_size, self._get_total_rows()):,} rows\")\n",
        "\n",
        "            df = pd.concat(chunks, ignore_index=True)\n",
        "            print(f\"Successfully loaded {len(df):,} rows\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            # Return empty DataFrame with expected structure\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def _get_total_rows(self):\n",
        "        \"\"\"Get total number of rows in the file\"\"\"\n",
        "        try:\n",
        "            return sum(1 for line in open(self.file_path)) - 1  # Subtract header\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Drop specified columns from the dataset\"\"\"\n",
        "    def __init__(self, columns_to_drop):\n",
        "        self.columns_to_drop = columns_to_drop\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Drop columns if they exist\n",
        "        columns_present = [col for col in self.columns_to_drop if col in X.columns]\n",
        "        X = X.drop(columns=columns_present)\n",
        "        return X\n",
        "\n",
        "class UpdatedSklearnDataPipeline:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.pipeline = None\n",
        "        self.processed_data = None\n",
        "        self.column_names = None\n",
        "        self._build_pipeline()\n",
        "\n",
        "    def _get_numeric_columns(self):\n",
        "        \"\"\"Define numeric columns for preprocessing\"\"\"\n",
        "        return ['Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',\n",
        "                'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',\n",
        "                'Amount_Paid_Each_Month_SGD', 'Missed_Payments_Count', 'Bounce_History',\n",
        "                'Contact_History_Call_Attempts', 'Contact_History_SMS', 'Contact_History_WhatsApp',\n",
        "                'Contact_History_EmailLogs', 'No_of_Attempts', 'Average_Handling_Time',\n",
        "                'Credit_Score', 'Recent_Inquiries', 'Loan_Exposure_Across_Banks',\n",
        "                'Recent_Score_Change', 'Unemployeement_rate_region', 'Inflation_Rate',\n",
        "                'Interest_Rate_Trend', 'Economic_Stress_Index', 'Income_Band_SGD',\n",
        "                'Utility_Spend_SGD', 'Shopping_Spend_SGD', 'Entertainment_Spend_SGD',\n",
        "                'Health_Spend_SGD', 'Education_Spend_SGD', 'Travel_Spend_SGD',\n",
        "                'Monthly_Spend_Trend_SGD', 'Seasonal_Spend_Variation', 'Weekend_Spend_Ratio',\n",
        "                'Festive_Season_Spend_SGD', 'Total_Monthly_Spend_SGD', 'Spend_to_Income_Ratio',\n",
        "                'UPI_Transaction_Count', 'Debit_Card_Transaction_Count', 'Credit_Card_Transaction_Count',\n",
        "                'Cash_Withdrawal_Count', 'Recurring_Transaction_Count', 'Recurring_Payment_Ratio',\n",
        "                'Savings_to_Spend_Ratio', 'Spend_Growth_Rate_YoY', 'High_Value_Transaction_Count',\n",
        "                'Flight_Risk_Score', 'Financial_Stress_Score', 'AAR_Score',\n",
        "                'Successful_Contacts_Count', 'Contact_Success_Rate', 'Customer_Best_Agent_Interaction_Count',\n",
        "                'App_Login_Frequency', 'Online_Banking_Activity', 'Monthly_Income_SGD']\n",
        "\n",
        "    def _get_categorical_columns(self):\n",
        "        \"\"\"Define categorical columns for preprocessing (Region will be encoded here)\"\"\"\n",
        "        return ['Product_Type', 'Payment_Frequency', 'Settlement_History',\n",
        "                'Channel_used', 'Response_Outcome', 'Urban_Rural_Tag',\n",
        "                'Language_Preference', 'Smartphone_Penetration', 'Preferred_Channel',\n",
        "                'Call_SMS_Activity_Patterns', 'WhatsApp_OTT_usage_Indicator',\n",
        "                'Regional_Time_Restrictions', 'Communication_Complaince_Limits',\n",
        "                'Gender', 'Occupation', 'Employeement_Type', 'Customer_Employment_Status',\n",
        "                'Finance_Stress_Status', 'Preferred_Payment_Channel', 'Financial_Health_Status',\n",
        "                'Avg_Balance_Trends', 'AAR_Risk_Level', 'Region']  # Region added for encoding\n",
        "\n",
        "    def _get_agent_columns(self):\n",
        "        \"\"\"Define agent columns that should be left as-is\"\"\"\n",
        "        return ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "\n",
        "    def _get_passthrough_columns(self):\n",
        "        \"\"\"Define columns that should be passed through without transformation\"\"\"\n",
        "        return ['Area_From_Address', 'Pincode_From_Address', 'Address']  # Updated to area and pincode\n",
        "\n",
        "    def _get_date_columns(self):\n",
        "        \"\"\"Define date columns for processing\"\"\"\n",
        "        return ['Installment_Due_Date', 'Last_Payment_Date']\n",
        "\n",
        "    def _get_columns_to_drop(self):\n",
        "        \"\"\"Define columns to drop from the dataset\"\"\"\n",
        "        return ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']\n",
        "\n",
        "    def _build_pipeline(self):\n",
        "        \"\"\"Build the updated pipeline\"\"\"\n",
        "\n",
        "        # Main preprocessing pipeline\n",
        "        self.pipeline = Pipeline([\n",
        "            # Step 1: Load data\n",
        "            ('data_loader', DataLoader(self.file_path)),\n",
        "\n",
        "            # Step 2: Drop phone number columns\n",
        "            ('column_dropper', ColumnDropper(self._get_columns_to_drop())),\n",
        "\n",
        "            # Step 3: Process address information (extract area and pincode but don't encode)\n",
        "            ('address_processor', AddressProcessor()),\n",
        "\n",
        "            # Step 4: Validate email addresses\n",
        "            ('email_validator', EmailValidator()),\n",
        "\n",
        "            # Step 5: Convert boolean columns\n",
        "            ('boolean_converter', BooleanConverter()),\n",
        "\n",
        "            # Step 6: Encode income bands\n",
        "            ('income_encoder', IncomeBandEncoder()),\n",
        "\n",
        "            # Step 7: Extract date features\n",
        "            ('date_extractor', DateFeatureExtractor(self._get_date_columns())),\n",
        "        ])\n",
        "\n",
        "    def fit_transform(self, save_path=None):\n",
        "        \"\"\"Run the complete pipeline and return processed data\"\"\"\n",
        "        print(\"=\" * 50)\n",
        "        print(\"RUNNING UPDATED SKLEARN DATA PIPELINE\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        try:\n",
        "            # Run the pipeline steps manually to maintain DataFrame structure\n",
        "            df = self.pipeline.named_steps['data_loader'].transform(None)\n",
        "            original_shape = df.shape\n",
        "\n",
        "            print(\"Applying preprocessing steps...\")\n",
        "            for step_name, transformer in list(self.pipeline.named_steps.items())[1:]:\n",
        "                print(f\"Applying {step_name}...\")\n",
        "                df = transformer.fit_transform(df)\n",
        "\n",
        "            # Remove duplicates\n",
        "            df = df.drop_duplicates()\n",
        "\n",
        "            # Apply standard preprocessing to categorical columns only\n",
        "            # Numeric columns will be handled in feature engineering\n",
        "            categorical_features = [col for col in self._get_categorical_columns() if col in df.columns]\n",
        "            agent_features = [col for col in self._get_agent_columns() if col in df.columns]\n",
        "            passthrough_features = [col for col in self._get_passthrough_columns() if col in df.columns]\n",
        "\n",
        "            # Handle categorical columns (including Region)\n",
        "            if categorical_features:\n",
        "                categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "                onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "                # Impute missing values\n",
        "                df[categorical_features] = categorical_imputer.fit_transform(df[categorical_features])\n",
        "\n",
        "                # One-hot encode categorical variables (including Region)\n",
        "                encoded_array = onehot_encoder.fit_transform(df[categorical_features])\n",
        "                encoded_columns = onehot_encoder.get_feature_names_out(categorical_features)\n",
        "\n",
        "                # Create DataFrame for encoded features\n",
        "                encoded_df = pd.DataFrame(encoded_array, columns=encoded_columns, index=df.index)\n",
        "\n",
        "                # Drop original categorical columns and add encoded ones\n",
        "                df = df.drop(columns=categorical_features)\n",
        "                df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "            # Agent columns, Area_From_Address, and Pincode_From_Address are left as-is (no transformation)\n",
        "            # They remain in the DataFrame\n",
        "\n",
        "            self.processed_data = df.reset_index(drop=True)\n",
        "\n",
        "            # Save if path provided\n",
        "            if save_path:\n",
        "                self.save_processed_data(save_path)\n",
        "\n",
        "            print(\"Pipeline completed successfully!\")\n",
        "            print(f\"Original shape: {original_shape}\")\n",
        "            print(f\"Processed shape: {self.processed_data.shape}\")\n",
        "\n",
        "            return self.processed_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in pipeline: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def save_processed_data(self, path):\n",
        "        \"\"\"Save the processed data\"\"\"\n",
        "        if self.processed_data is not None:\n",
        "            self.processed_data.to_csv(path, index=False)\n",
        "            print(f\"Processed data saved to {path}\")\n",
        "\n",
        "            # Also save the pipeline for future use\n",
        "            joblib.dump(self.pipeline, 'data_pipeline.pkl')\n",
        "            print(\"Pipeline saved as 'data_pipeline.pkl'\")\n",
        "\n",
        "    def load_and_transform_new_data(self, new_data_path):\n",
        "        \"\"\"Load and transform new data using the fitted pipeline\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Pipeline not fitted yet. Call fit_transform first.\")\n",
        "\n",
        "        # Load new data\n",
        "        new_data_loader = DataLoader(new_data_path)\n",
        "        new_df = new_data_loader.transform(None)\n",
        "\n",
        "        # Apply the same transformations\n",
        "        for step_name, transformer in list(self.pipeline.named_steps.items())[1:]:\n",
        "            new_df = transformer.transform(new_df)\n",
        "\n",
        "        return new_df\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the updated sklearn pipeline\n",
        "    updated_pipeline = UpdatedSklearnDataPipeline('singapore_loan_data.csv')\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    processed_data = updated_pipeline.fit_transform(save_path='processed_data.csv')\n",
        "\n",
        "    if processed_data is not None:\n",
        "        # Display results\n",
        "        print(\"\\nPipeline Summary:\")\n",
        "        print(f\"Processed data shape: {processed_data.shape}\")\n",
        "        print(f\"Processed data columns: {len(processed_data.columns)}\")\n",
        "\n",
        "        print(\"\\nSample of processed data:\")\n",
        "        print(processed_data.head())\n",
        "\n",
        "        print(\"\\nData types after processing:\")\n",
        "        print(processed_data.dtypes.value_counts())\n",
        "\n",
        "        # Show the new columns created by the data cleaning processes\n",
        "        new_columns = ['Area_From_Address', 'Pincode_From_Address', 'Email_Valid_Format', 'Email_Domain',\n",
        "                       'Email_Domain_Legitimate', 'Email_Disposable']\n",
        "\n",
        "        print(\"\\nNew columns created by data cleaning:\")\n",
        "        for col in new_columns:\n",
        "            if col in processed_data.columns:\n",
        "                print(f\"{col}: {processed_data[col].notna().sum()} non-null values\")\n",
        "\n",
        "        # Show date features created\n",
        "        date_features = ['Installment_Due_Date_month', 'Installment_Due_Date_dayofweek',\n",
        "                        'Installment_Due_Date_year', 'Last_Payment_Date_month',\n",
        "                        'Last_Payment_Date_dayofweek', 'Last_Payment_Date_year']\n",
        "\n",
        "        print(\"\\nDate features created:\")\n",
        "        for col in date_features:\n",
        "            if col in processed_data.columns:\n",
        "                print(f\"{col}: {processed_data[col].notna().sum()} non-null values\")\n",
        "\n",
        "        # Show agent columns are preserved as-is\n",
        "        agent_columns = ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "        print(\"\\nAgent columns preserved as-is:\")\n",
        "        for col in agent_columns:\n",
        "            if col in processed_data.columns:\n",
        "                print(f\"{col}: {processed_data[col].notna().sum()} non-null values\")\n",
        "                print(f\"Sample values: {processed_data[col].head(3).tolist()}\")\n",
        "\n",
        "        # Show Region encoding\n",
        "        region_encoded_columns = [col for col in processed_data.columns if col.startswith('Region_')]\n",
        "        print(f\"\\nRegion encoded columns ({len(region_encoded_columns)}):\")\n",
        "        for col in region_encoded_columns[:5]:  # Show first 5\n",
        "            print(f\"  {col}: {processed_data[col].sum()} records\")\n",
        "        if len(region_encoded_columns) > 5:\n",
        "            print(f\"  ... and {len(region_encoded_columns) - 5} more region columns\")\n",
        "\n",
        "        # Show Area_From_Address and Pincode_From_Address are preserved as-is (not encoded)\n",
        "        if 'Area_From_Address' in processed_data.columns:\n",
        "            print(f\"\\nArea_From_Address (preserved as-is):\")\n",
        "            print(f\"  Unique values: {processed_data['Area_From_Address'].nunique()}\")\n",
        "            print(f\"  Sample values: {processed_data['Area_From_Address'].head(5).tolist()}\")\n",
        "\n",
        "        if 'Pincode_From_Address' in processed_data.columns:\n",
        "            print(f\"\\nPincode_From_Address (preserved as-is):\")\n",
        "            print(f\"  Unique values: {processed_data['Pincode_From_Address'].nunique()}\")\n",
        "            print(f\"  Sample values: {processed_data['Pincode_From_Address'].head(5).tolist()}\")\n",
        "\n",
        "        # Verify phone number columns are removed\n",
        "        phone_columns = ['Primary_Phone_Number', 'Secondary_Mobile_Number', 'Landline_Phone_Number']\n",
        "        print(f\"\\nPhone number columns removed:\")\n",
        "        for col in phone_columns:\n",
        "            if col not in processed_data.columns:\n",
        "                print(f\"  âœ“ {col} successfully removed\")\n",
        "    else:\n",
        "        print(\"Pipeline failed to process data.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "import joblib\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class NumericPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Handle numeric columns imputation and scaling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.numeric_imputer = None\n",
        "        self.numeric_scaler = None\n",
        "        self.numeric_columns_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Define numeric columns\n",
        "        self.numeric_columns_ = [\n",
        "            'Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',\n",
        "            'Tenure', 'Interest_Rate', 'Current_EMI_SGD', 'Number_of_Past_Payments',\n",
        "            'Amount_Paid_Each_Month_SGD', 'Missed_Payments_Count', 'Bounce_History',\n",
        "            'Contact_History_Call_Attempts', 'Contact_History_SMS', 'Contact_History_WhatsApp',\n",
        "            'Contact_History_EmailLogs', 'No_of_Attempts', 'Average_Handling_Time',\n",
        "            'Credit_Score', 'Recent_Inquiries', 'Loan_Exposure_Across_Banks',\n",
        "            'Recent_Score_Change', 'Unemployeement_rate_region', 'Inflation_Rate',\n",
        "            'Interest_Rate_Trend', 'Economic_Stress_Index', 'Income_Band_SGD',\n",
        "            'Utility_Spend_SGD', 'Shopping_Spend_SGD', 'Entertainment_Spend_SGD',\n",
        "            'Health_Spend_SGD', 'Education_Spend_SGD', 'Travel_Spend_SGD',\n",
        "            'Monthly_Spend_Trend_SGD', 'Seasonal_Spend_Variation', 'Weekend_Spend_Ratio',\n",
        "            'Festive_Season_Spend_SGD', 'Total_Monthly_Spend_SGD', 'Spend_to_Income_Ratio',\n",
        "            'UPI_Transaction_Count', 'Debit_Card_Transaction_Count', 'Credit_Card_Transaction_Count',\n",
        "            'Cash_Withdrawal_Count', 'Recurring_Transaction_Count', 'Recurring_Payment_Ratio',\n",
        "            'Savings_to_Spend_Ratio', 'Spend_Growth_Rate_YoY', 'High_Value_Transaction_Count',\n",
        "            'Flight_Risk_Score', 'Financial_Stress_Score', 'AAR_Score',\n",
        "            'Successful_Contacts_Count', 'Contact_Success_Rate', 'Customer_Best_Agent_Interaction_Count',\n",
        "            'App_Login_Frequency', 'Online_Banking_Activity', 'Monthly_Income_SGD'\n",
        "        ]\n",
        "\n",
        "        # Filter to only columns that exist in the data\n",
        "        available_numeric_cols = [col for col in self.numeric_columns_ if col in X.columns]\n",
        "\n",
        "        if available_numeric_cols:\n",
        "            self.numeric_imputer = SimpleImputer(strategy='median')\n",
        "            self.numeric_scaler = StandardScaler()\n",
        "\n",
        "            # Fit on available numeric data\n",
        "            numeric_data = X[available_numeric_cols]\n",
        "            self.numeric_imputer.fit(numeric_data)\n",
        "            self.numeric_scaler.fit(numeric_data)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Filter to only columns that exist in the data\n",
        "        available_numeric_cols = [col for col in self.numeric_columns_ if col in X.columns]\n",
        "\n",
        "        if available_numeric_cols and self.numeric_imputer is not None:\n",
        "            print(f\"ðŸ”§ Processing {len(available_numeric_cols)} numeric columns...\")\n",
        "\n",
        "            # Impute missing values\n",
        "            X[available_numeric_cols] = self.numeric_imputer.transform(X[available_numeric_cols])\n",
        "\n",
        "            # Scale numeric features\n",
        "            X[available_numeric_cols] = self.numeric_scaler.transform(X[available_numeric_cols])\n",
        "\n",
        "            print(f\"âœ… Numeric preprocessing completed: imputation + scaling\")\n",
        "\n",
        "        return X\n",
        "\n",
        "class FinancialRiskCalculator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Calculate comprehensive financial risk scores and ratios\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Debt Service Coverage Ratio (DSCR)\n",
        "        if all(col in X.columns for col in ['Monthly_Income_SGD', 'Current_EMI_SGD']):\n",
        "            X['DSCR'] = X['Monthly_Income_SGD'] / (X['Current_EMI_SGD'] + 1e-8)\n",
        "\n",
        "        # Loan-to-Income Ratio\n",
        "        if all(col in X.columns for col in ['Loan_Amount_SGD', 'Monthly_Income_SGD']):\n",
        "            X['Loan_to_Income_Ratio'] = X['Loan_Amount_SGD'] / (X['Monthly_Income_SGD'] * 12 + 1e-8)\n",
        "\n",
        "        # Credit Utilization Ratio\n",
        "        if all(col in X.columns for col in ['Outstanding_Balance_SGD', 'Loan_Amount_SGD']):\n",
        "            X['Credit_Utilization_Ratio'] = X['Outstanding_Balance_SGD'] / (X['Loan_Amount_SGD'] + 1e-8)\n",
        "\n",
        "        # Payment Efficiency Score\n",
        "        if all(col in X.columns for col in ['Number_of_Past_Payments', 'Tenure', 'Missed_Payments_Count']):\n",
        "            total_expected_payments = X['Tenure'] * 12  # Assuming monthly payments\n",
        "            X['Payment_Efficiency'] = (X['Number_of_Past_Payments'] - X['Missed_Payments_Count']) / (total_expected_payments + 1e-8)\n",
        "\n",
        "        # Financial Stress Composite Score\n",
        "        stress_indicators = []\n",
        "        if 'Financial_Stress_Score' in X.columns:\n",
        "            stress_indicators.append(X['Financial_Stress_Score'])\n",
        "        if 'Day_Past_Due' in X.columns:\n",
        "            stress_indicators.append(X['Day_Past_Due'] / 100)  # Normalize\n",
        "        if 'Missed_Payments_Count' in X.columns:\n",
        "            stress_indicators.append(X['Missed_Payments_Count'] / 12)  # Normalize by year\n",
        "\n",
        "        if stress_indicators:\n",
        "            X['Composite_Financial_Stress_Score'] = pd.concat(stress_indicators, axis=1).mean(axis=1)\n",
        "\n",
        "        # Behavioral Spending Pattern\n",
        "        if all(col in X.columns for col in ['Total_Monthly_Spend_SGD', 'Monthly_Income_SGD']):\n",
        "            X['Spending_Behavior_Ratio'] = X['Total_Monthly_Spend_SGD'] / (X['Monthly_Income_SGD'] + 1e-8)\n",
        "\n",
        "        # Liquidity Score\n",
        "        if all(col in X.columns for col in ['Savings_to_Spend_Ratio', 'High_Value_Transaction_Count']):\n",
        "            X['Liquidity_Score'] = (X['Savings_to_Spend_Ratio'] +\n",
        "                                   X['High_Value_Transaction_Count'] / 100)  # Normalize\n",
        "\n",
        "        return X\n",
        "\n",
        "class CustomerSegmentationEngine(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Advanced customer segmentation using RFM and financial behavior\"\"\"\n",
        "\n",
        "    def __init__(self, n_segments=5):\n",
        "        self.n_segments = n_segments\n",
        "        self.kmeans = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Prepare features for segmentation\n",
        "        segmentation_features = self._prepare_segmentation_features(X)\n",
        "        if len(segmentation_features) > 0:\n",
        "            self.kmeans = KMeans(n_clusters=self.n_segments, random_state=42)\n",
        "            self.kmeans.fit(segmentation_features)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # RFM-like segments for loan customers\n",
        "        # Recency: Days since last payment (inverse of Day_Past_Due)\n",
        "        if 'Day_Past_Due' in X.columns:\n",
        "            X['Recency_Score'] = 1 / (X['Day_Past_Due'] + 1)\n",
        "\n",
        "        # Frequency Score (payment regularity)\n",
        "        if all(col in X.columns for col in ['Number_of_Past_Payments', 'Missed_Payments_Count']):\n",
        "            X['Frequency_Score'] = X['Number_of_Past_Payments'] / (X['Number_of_Past_Payments'] + X['Missed_Payments_Count'] + 1e-8)\n",
        "\n",
        "        # Monetary Score (financial capacity)\n",
        "        monetary_components = []\n",
        "        if 'Loan_Amount_SGD' in X.columns:\n",
        "            monetary_components.append(X['Loan_Amount_SGD'])\n",
        "        if 'Monthly_Income_SGD' in X.columns:\n",
        "            monetary_components.append(X['Monthly_Income_SGD'])\n",
        "        if 'Total_Monthly_Spend_SGD' in X.columns:\n",
        "            monetary_components.append(X['Total_Monthly_Spend_SGD'])\n",
        "\n",
        "        if monetary_components:\n",
        "            X['Monetary_Score'] = pd.concat(monetary_components, axis=1).mean(axis=1)\n",
        "\n",
        "        # RFM Composite Score\n",
        "        rfm_components = [col for col in ['Recency_Score', 'Frequency_Score', 'Monetary_Score'] if col in X.columns]\n",
        "        if rfm_components:\n",
        "            X['RFM_Score'] = pd.concat([X[col] for col in rfm_components], axis=1).mean(axis=1)\n",
        "\n",
        "        # Behavioral Segments\n",
        "        conditions = [\n",
        "            # High Risk Delinquent\n",
        "            (X.get('Day_Past_Due', 0) > 30) & (X.get('Financial_Stress_Score', 0) > 0.7),\n",
        "\n",
        "            # Stable Payers\n",
        "            (X.get('Day_Past_Due', 0) <= 0) & (X.get('Payment_Efficiency', 0) > 0.9),\n",
        "\n",
        "            # High Value Customers\n",
        "            (X.get('Loan_Amount_SGD', 0) > X['Loan_Amount_SGD'].quantile(0.75)) &\n",
        "            (X.get('Monthly_Income_SGD', 0) > X['Monthly_Income_SGD'].quantile(0.75)),\n",
        "\n",
        "            # Digital Savvy\n",
        "            (X.get('App_Login_Frequency', 0) > 0.7) &\n",
        "            (X.get('Online_Banking_Activity', 0) > 0.7),\n",
        "\n",
        "            # Traditional Customers\n",
        "            (X.get('App_Login_Frequency', 0) < 0.3) &\n",
        "            (X.get('Online_Banking_Activity', 0) < 0.3)\n",
        "        ]\n",
        "\n",
        "        segments = ['High_Risk_Delinquent', 'Stable_Payer', 'High_Value', 'Digital_Savvy', 'Traditional']\n",
        "\n",
        "        X['Behavioral_Segment'] = 'Standard'\n",
        "        for condition, segment in zip(conditions, segments):\n",
        "            if isinstance(condition, pd.Series):\n",
        "                X.loc[condition, 'Behavioral_Segment'] = segment\n",
        "\n",
        "        # Apply K-means clustering if fitted\n",
        "        if self.kmeans is not None:\n",
        "            segmentation_features = self._prepare_segmentation_features(X)\n",
        "            if len(segmentation_features) > 0:\n",
        "                X['KMeans_Cluster'] = self.kmeans.predict(segmentation_features)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _prepare_segmentation_features(self, X):\n",
        "        features = []\n",
        "        numeric_cols = ['Loan_Amount_SGD', 'Monthly_Income_SGD', 'Day_Past_Due',\n",
        "                       'Credit_Score', 'Financial_Stress_Score', 'App_Login_Frequency']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in X.columns:\n",
        "                # Handle potential missing values\n",
        "                features.append(X[col].fillna(0))\n",
        "\n",
        "        if features:\n",
        "            return pd.concat(features, axis=1)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "class ChannelEffectivenessCalculator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Calculate channel effectiveness and customer responsiveness\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Channel Response Success Rate\n",
        "        channel_columns = {\n",
        "            'Call': ['Contact_History_Call_Attempts', 'Channel_used_Call'],\n",
        "            'SMS': ['Contact_History_SMS', 'Channel_used_SMS'],\n",
        "            'WhatsApp': ['Contact_History_WhatsApp', 'Channel_used_WhatsApp'],\n",
        "            'Email': ['Contact_History_EmailLogs', 'Channel_used_Email']\n",
        "        }\n",
        "\n",
        "        for channel, (attempts_col, usage_col) in channel_columns.items():\n",
        "            if all(col in X.columns for col in [attempts_col, usage_col]):\n",
        "                if 'Successful_Contacts_Count' in X.columns:\n",
        "                    # Channel Efficiency (success per attempt)\n",
        "                    X[f'{channel}_Efficiency'] = X['Successful_Contacts_Count'] / (X[attempts_col] + 1e-8)\n",
        "\n",
        "                    # Channel Preference Strength\n",
        "                    X[f'{channel}_Preference_Strength'] = X[usage_col] * X.get(f'{channel}_Efficiency', 0)\n",
        "\n",
        "        # Multi-Channel Engagement Score\n",
        "        channel_efficiency_cols = [f'{channel}_Efficiency' for channel in channel_columns.keys()\n",
        "                                 if f'{channel}_Efficiency' in X.columns]\n",
        "        if channel_efficiency_cols:\n",
        "            X['Multi_Channel_Engagement_Score'] = pd.concat([X[col] for col in channel_efficiency_cols], axis=1).mean(axis=1)\n",
        "\n",
        "        # Time-Based Responsiveness\n",
        "        if all(col in X.columns for col in ['Contact_Success_Rate', 'Average_Handling_Time']):\n",
        "            X['Time_Adjusted_Responsiveness'] = X['Contact_Success_Rate'] / (X['Average_Handling_Time'] + 1e-8)\n",
        "\n",
        "        # Agent Effectiveness - PRESERVING AGENT ID SECTIONS\n",
        "        if all(col in X.columns for col in ['Customer_Best_Agent_Interaction_Count', 'Contact_History_Call_Attempts']):\n",
        "            X['Agent_Effectiveness_Score'] = X['Customer_Best_Agent_Interaction_Count'] / (\n",
        "                X['Contact_History_Call_Attempts'] + 1e-8)\n",
        "\n",
        "        # Digital vs Traditional Preference\n",
        "        digital_channels = ['Channel_used_Email', 'Channel_used_WhatsApp', 'Channel_used_SMS']\n",
        "        traditional_channels = ['Channel_used_Call', 'Channel_used_IVR', 'Channel_used_Field Agent']\n",
        "\n",
        "        digital_cols = [col for col in digital_channels if col in X.columns]\n",
        "        traditional_cols = [col for col in traditional_channels if col in X.columns]\n",
        "\n",
        "        if digital_cols and traditional_cols:\n",
        "            digital_pref = pd.concat([X[col] for col in digital_cols], axis=1).mean(axis=1)\n",
        "            traditional_pref = pd.concat([X[col] for col in traditional_cols], axis=1).mean(axis=1)\n",
        "            X['Digital_vs_Traditional_Preference'] = digital_pref - traditional_pref\n",
        "\n",
        "        return X\n",
        "\n",
        "class TemporalPatternEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Engineer temporal and seasonal patterns\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Payment Cycle Patterns\n",
        "        if 'Installment_Due_Date_dayofweek' in X.columns:\n",
        "            # Weekend vs Weekday payment preference\n",
        "            X['Weekend_Payment_Preference'] = X['Installment_Due_Date_dayofweek'].isin([5, 6]).astype(int)\n",
        "\n",
        "        # Seasonal Payment Patterns\n",
        "        if 'Installment_Due_Date_month' in X.columns:\n",
        "            # Quarter-based segmentation\n",
        "            X['Payment_Quarter'] = (X['Installment_Due_Date_month'] - 1) // 3 + 1\n",
        "\n",
        "            # Festive season (year-end)\n",
        "            X['Festive_Season_Payment'] = X['Installment_Due_Date_month'].isin([11, 12]).astype(int)\n",
        "\n",
        "        # Recency of Last Payment\n",
        "        if 'Last_Payment_Date_month' in X.columns and 'Installment_Due_Date_month' in X.columns:\n",
        "            X['Months_Since_Last_Payment'] = X['Installment_Due_Date_month'] - X['Last_Payment_Date_month']\n",
        "            X['Months_Since_Last_Payment'] = X['Months_Since_Last_Payment'].apply(\n",
        "                lambda x: x if x >= 0 else x + 12\n",
        "            )\n",
        "\n",
        "        # Payment Regularity Score\n",
        "        if all(col in X.columns for col in ['Number_of_Past_Payments', 'Tenure']):\n",
        "            expected_payments = X['Tenure'] * 12  # Assuming monthly payments\n",
        "            X['Payment_Regularity_Score'] = X['Number_of_Past_Payments'] / (expected_payments + 1e-8)\n",
        "\n",
        "        return X\n",
        "\n",
        "class AdvancedDigitalBehaviorEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Engineer advanced digital behavior features\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Digital Engagement Composite Score\n",
        "        digital_metrics = []\n",
        "        if 'App_Login_Frequency' in X.columns:\n",
        "            digital_metrics.append(X['App_Login_Frequency'])\n",
        "        if 'Online_Banking_Activity' in X.columns:\n",
        "            digital_metrics.append(X['Online_Banking_Activity'])\n",
        "        if 'UPI_Transaction_Count' in X.columns:\n",
        "            digital_metrics.append(X['UPI_Transaction_Count'] / 100)  # Normalize\n",
        "\n",
        "        if digital_metrics:\n",
        "            X['Digital_Engagement_Score'] = pd.concat(digital_metrics, axis=1).mean(axis=1)\n",
        "\n",
        "        # Transaction Diversity Score\n",
        "        transaction_types = ['UPI_Transaction_Count', 'Debit_Card_Transaction_Count',\n",
        "                           'Credit_Card_Transaction_Count', 'Cash_Withdrawal_Count']\n",
        "\n",
        "        available_transactions = [col for col in transaction_types if col in X.columns]\n",
        "        if available_transactions:\n",
        "            transaction_matrix = X[available_transactions]\n",
        "            X['Transaction_Diversity_Score'] = (transaction_matrix > 0).sum(axis=1) / len(available_transactions)\n",
        "\n",
        "        # Channel Adaptability Score\n",
        "        channel_columns = ['Channel_used_Call', 'Channel_used_SMS', 'Channel_used_WhatsApp',\n",
        "                         'Channel_used_Email', 'Channel_used_IVR', 'Channel_used_Field Agent']\n",
        "\n",
        "        available_channels = [col for col in channel_columns if col in X.columns]\n",
        "        if available_channels:\n",
        "            X['Channel_Adaptability_Score'] = (X[available_channels] > 0).sum(axis=1) / len(available_channels)\n",
        "\n",
        "        # Communication Responsiveness\n",
        "        if all(col in X.columns for col in ['Contact_Success_Rate', 'No_of_Attempts']):\n",
        "            X['Communication_Responsiveness'] = X['Contact_Success_Rate'] / (X['No_of_Attempts'] + 1e-8)\n",
        "\n",
        "        return X\n",
        "\n",
        "class AgentIdPreserver(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Preserve agent ID columns throughout the pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.agent_columns = ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Agent columns are automatically preserved as they remain in the DataFrame\n",
        "        return X\n",
        "\n",
        "class FeatureOptimizer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Optimize and select the best features for channel ranking prediction\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.best_features = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Define the optimal feature set based on domain knowledge\n",
        "        self.best_features = [\n",
        "            # Customer Identity\n",
        "            'Customer_id',\n",
        "\n",
        "            # Financial Risk Features\n",
        "            'DSCR', 'Loan_to_Income_Ratio', 'Credit_Utilization_Ratio',\n",
        "            'Payment_Efficiency', 'Composite_Financial_Stress_Score',\n",
        "            'Spending_Behavior_Ratio', 'Liquidity_Score',\n",
        "\n",
        "            # Customer Segmentation\n",
        "            'Recency_Score', 'Frequency_Score', 'Monetary_Score', 'RFM_Score',\n",
        "            'Behavioral_Segment', 'KMeans_Cluster',\n",
        "\n",
        "            # Channel Effectiveness\n",
        "            'Call_Efficiency', 'SMS_Efficiency', 'WhatsApp_Efficiency', 'Email_Efficiency',\n",
        "            'Multi_Channel_Engagement_Score', 'Time_Adjusted_Responsiveness',\n",
        "            'Agent_Effectiveness_Score', 'Digital_vs_Traditional_Preference',\n",
        "            'Channel_Adaptability_Score', 'Communication_Responsiveness',\n",
        "\n",
        "            # Temporal Patterns\n",
        "            'Weekend_Payment_Preference', 'Payment_Quarter',\n",
        "            'Festive_Season_Payment', 'Months_Since_Last_Payment',\n",
        "            'Payment_Regularity_Score',\n",
        "\n",
        "            # Digital Behavior\n",
        "            'Digital_Engagement_Score', 'Transaction_Diversity_Score',\n",
        "\n",
        "            # Agent Information - PRESERVED\n",
        "            'Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs',\n",
        "\n",
        "            # Original Important Features (filtered)\n",
        "            'Age', 'Loan_Amount_SGD', 'Outstanding_Balance_SGD', 'Day_Past_Due',\n",
        "            'Credit_Score', 'Monthly_Income_SGD', 'Financial_Stress_Score',\n",
        "            'Contact_Success_Rate', 'App_Login_Frequency', 'Online_Banking_Activity'\n",
        "        ]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Select only features that exist in the dataset\n",
        "        available_features = [f for f in self.best_features if f in X.columns]\n",
        "\n",
        "        # Ensure Customer_id is always included\n",
        "        if 'Customer_id' in X.columns and 'Customer_id' not in available_features:\n",
        "            available_features = ['Customer_id'] + available_features\n",
        "\n",
        "        # Always include agent columns if they exist\n",
        "        agent_columns = ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "        for agent_col in agent_columns:\n",
        "            if agent_col in X.columns and agent_col not in available_features:\n",
        "                available_features.append(agent_col)\n",
        "\n",
        "        return X[available_features]\n",
        "\n",
        "class ComprehensiveFeaturePipeline:\n",
        "    \"\"\"Comprehensive feature engineering pipeline for channel ranking prediction\"\"\"\n",
        "\n",
        "    def __init__(self, n_segments=5):\n",
        "        self.n_segments = n_segments\n",
        "        self.pipeline = None\n",
        "        self.feature_names = None\n",
        "        self._build_pipeline()\n",
        "\n",
        "    def _build_pipeline(self):\n",
        "        \"\"\"Build the comprehensive feature engineering pipeline\"\"\"\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            # Step 0: Agent ID Preserver\n",
        "            ('agent_preserver', AgentIdPreserver()),\n",
        "\n",
        "            # Step 1: Numeric Preprocessing (Imputation + Scaling)\n",
        "            ('numeric_preprocessor', NumericPreprocessor()),\n",
        "\n",
        "            # Step 2: Financial Risk Assessment\n",
        "            ('financial_risk_calculator', FinancialRiskCalculator()),\n",
        "\n",
        "            # Step 3: Customer Segmentation\n",
        "            ('customer_segmentation', CustomerSegmentationEngine(n_segments=self.n_segments)),\n",
        "\n",
        "            # Step 4: Channel Effectiveness Analysis\n",
        "            ('channel_effectiveness', ChannelEffectivenessCalculator()),\n",
        "\n",
        "            # Step 5: Temporal Pattern Analysis\n",
        "            ('temporal_engineer', TemporalPatternEngineer()),\n",
        "\n",
        "            # Step 6: Digital Behavior Analysis\n",
        "            ('digital_behavior_engineer', AdvancedDigitalBehaviorEngineer()),\n",
        "\n",
        "            # Step 7: Feature Optimization\n",
        "            ('feature_optimizer', FeatureOptimizer())\n",
        "        ])\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"COMPREHENSIVE FEATURE ENGINEERING PIPELINE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Store original Customer_id and Agent IDs\n",
        "            original_columns = {}\n",
        "            important_columns = ['Customer_id', 'Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "\n",
        "            for col in important_columns:\n",
        "                if col in X.columns:\n",
        "                    original_columns[col] = X[col].copy()\n",
        "\n",
        "            # Transform data\n",
        "            print(\"ðŸ”„ Applying feature engineering steps...\")\n",
        "            X_transformed = self.pipeline.fit_transform(X)\n",
        "\n",
        "            # Ensure important columns are preserved\n",
        "            for col, values in original_columns.items():\n",
        "                if col not in X_transformed.columns:\n",
        "                    X_transformed[col] = values\n",
        "\n",
        "            # Move Customer_id to first column\n",
        "            if 'Customer_id' in X_transformed.columns:\n",
        "                cols = ['Customer_id'] + [col for col in X_transformed.columns if col != 'Customer_id']\n",
        "                X_transformed = X_transformed[cols]\n",
        "\n",
        "            self.feature_names = list(X_transformed.columns)\n",
        "\n",
        "            print(\"âœ… Feature engineering completed successfully!\")\n",
        "            print(f\"ðŸ“Š Original shape: {X.shape}\")\n",
        "            print(f\"ðŸ“ˆ Engineered shape: {X_transformed.shape}\")\n",
        "            print(f\"ðŸŽ¯ Number of features: {len(self.feature_names)}\")\n",
        "\n",
        "            # Show feature categories\n",
        "            self._analyze_feature_categories(X_transformed)\n",
        "\n",
        "            return X_transformed\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in feature pipeline: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def _analyze_feature_categories(self, X):\n",
        "        \"\"\"Analyze and display feature categories\"\"\"\n",
        "        feature_categories = {\n",
        "            'Financial Risk': ['DSCR', 'Loan_to_Income_Ratio', 'Credit_Utilization_Ratio',\n",
        "                              'Payment_Efficiency', 'Composite_Financial_Stress_Score'],\n",
        "            'Customer Segmentation': ['Recency_Score', 'Frequency_Score', 'Monetary_Score',\n",
        "                                    'RFM_Score', 'Behavioral_Segment', 'KMeans_Cluster'],\n",
        "            'Channel Effectiveness': ['Call_Efficiency', 'SMS_Efficiency', 'WhatsApp_Efficiency',\n",
        "                                     'Multi_Channel_Engagement_Score', 'Agent_Effectiveness_Score'],\n",
        "            'Temporal Patterns': ['Weekend_Payment_Preference', 'Payment_Quarter',\n",
        "                                'Months_Since_Last_Payment', 'Payment_Regularity_Score'],\n",
        "            'Digital Behavior': ['Digital_Engagement_Score', 'Transaction_Diversity_Score',\n",
        "                               'Channel_Adaptability_Score'],\n",
        "            'Agent Information': ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"FEATURE CATEGORY ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for category, features in feature_categories.items():\n",
        "            available_features = [f for f in features if f in X.columns]\n",
        "            if available_features:\n",
        "                print(f\"ðŸ“ {category}: {len(available_features)} features\")\n",
        "                for feature in available_features[:3]:  # Show first 3\n",
        "                    print(f\"   â”œâ”€â”€ {feature}\")\n",
        "                if len(available_features) > 3:\n",
        "                    print(f\"   â””â”€â”€ ... and {len(available_features) - 3} more\")\n",
        "\n",
        "        # Show total engineered features\n",
        "        engineered_keywords = ['_Score', '_Ratio', '_Efficiency', '_Segment']\n",
        "        engineered_features = [col for col in X.columns if any(keyword in col for keyword in engineered_keywords)]\n",
        "        print(f\"\\nðŸŽ¯ Total engineered features: {len(engineered_features)}\")\n",
        "\n",
        "        # Verify agent columns are preserved\n",
        "        agent_columns = [col for col in ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs'] if col in X.columns]\n",
        "        if agent_columns:\n",
        "            print(f\"ðŸ‘¤ Agent columns preserved: {len(agent_columns)}\")\n",
        "            for agent_col in agent_columns:\n",
        "                print(f\"   âœ… {agent_col}\")\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform new data using fitted pipeline\"\"\"\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Pipeline not fitted yet. Call fit_transform first.\")\n",
        "\n",
        "        # Store original important columns\n",
        "        original_columns = {}\n",
        "        important_columns = ['Customer_id', 'Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "\n",
        "        for col in important_columns:\n",
        "            if col in X.columns:\n",
        "                original_columns[col] = X[col].copy()\n",
        "\n",
        "        X_transformed = self.pipeline.transform(X)\n",
        "\n",
        "        # Ensure important columns are preserved\n",
        "        for col, values in original_columns.items():\n",
        "            if col not in X_transformed.columns:\n",
        "                X_transformed[col] = values\n",
        "\n",
        "        if 'Customer_id' in X_transformed.columns:\n",
        "            cols = ['Customer_id'] + [col for col in X_transformed.columns if col != 'Customer_id']\n",
        "            X_transformed = X_transformed[cols]\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "    def get_feature_importance_analysis(self):\n",
        "        \"\"\"Provide domain-driven feature importance analysis\"\"\"\n",
        "        importance_analysis = {\n",
        "            'CRITICAL': [\n",
        "                'DSCR', 'Loan_to_Income_Ratio', 'Day_Past_Due', 'Payment_Efficiency',\n",
        "                'Composite_Financial_Stress_Score'\n",
        "            ],\n",
        "            'HIGH': [\n",
        "                'RFM_Score', 'Behavioral_Segment', 'Multi_Channel_Engagement_Score',\n",
        "                'Channel_Adaptability_Score'\n",
        "            ],\n",
        "            'MEDIUM': [\n",
        "                'Digital_Engagement_Score', 'Communication_Responsiveness',\n",
        "                'Payment_Regularity_Score', 'Transaction_Diversity_Score'\n",
        "            ],\n",
        "            'CONTEXTUAL': [\n",
        "                'Weekend_Payment_Preference', 'Festive_Season_Payment',\n",
        "                'Digital_vs_Traditional_Preference', 'Agent_Effectiveness_Score'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"DOMAIN EXPERT FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for importance, features in importance_analysis.items():\n",
        "            available_features = [f for f in features if f in self.feature_names]\n",
        "            if available_features:\n",
        "                print(f\"\\n{importance} IMPORTANCE:\")\n",
        "                for feature in available_features:\n",
        "                    print(f\"  âœ… {feature}\")\n",
        "\n",
        "# Example usage with your data\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your preprocessed data\n",
        "    df = pd.read_csv('processed_data.csv')\n",
        "\n",
        "    # Ensure Customer_id exists\n",
        "    if 'Customer_id' not in df.columns:\n",
        "        print(\"âš ï¸ Creating temporary Customer_id...\")\n",
        "        df['Customer_id'] = [f'CUST_{i+1:06d}' for i in range(len(df))]\n",
        "\n",
        "    # Initialize comprehensive feature pipeline\n",
        "    feature_pipeline = ComprehensiveFeaturePipeline(n_segments=5)\n",
        "\n",
        "    # Run feature engineering\n",
        "    engineered_features = feature_pipeline.fit_transform(df)\n",
        "\n",
        "    if engineered_features is not None:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"FINAL ENGINEERED FEATURES SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"âœ… Final dataset shape: {engineered_features.shape}\")\n",
        "        print(f\"âœ… Total features: {len(engineered_features.columns)}\")\n",
        "\n",
        "        # Display sample of engineered features\n",
        "        print(\"\\nðŸ“Š Sample of engineered features (first 10 columns):\")\n",
        "        print(engineered_features.iloc[:, :10].head())\n",
        "\n",
        "        # Show feature importance analysis\n",
        "        feature_pipeline.get_feature_importance_analysis()\n",
        "\n",
        "        # Save results\n",
        "        engineered_features.to_csv(\"comprehensive_engineered_features.csv\", index=False)\n",
        "        print(\"\\nðŸ’¾ Engineered features saved to 'comprehensive_engineered_features.csv'\")\n",
        "\n",
        "        joblib.dump(feature_pipeline, \"comprehensive_feature_pipeline.pkl\")\n",
        "        print(\"ðŸ’¾ Feature pipeline saved to 'comprehensive_feature_pipeline.pkl'\")\n",
        "\n",
        "        # Generate feature report\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"FEATURE ENGINEERING REPORT\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Count features by type\n",
        "        financial_features = [col for col in engineered_features.columns if any(keyword in col for keyword in ['Ratio', 'Score', 'Efficiency'])]\n",
        "        segment_features = [col for col in engineered_features.columns if 'Segment' in col or 'Cluster' in col]\n",
        "        agent_features = [col for col in engineered_features.columns if 'Agent' in col]\n",
        "\n",
        "        print(f\"ðŸ’° Financial Risk Features: {len(financial_features)}\")\n",
        "        print(f\"ðŸ‘¥ Customer Segmentation Features: {len(segment_features)}\")\n",
        "        print(f\"ðŸ“± Channel Effectiveness Features: {len([col for col in engineered_features.columns if 'Channel' in col])}\")\n",
        "        print(f\"ðŸ•’ Temporal Pattern Features: {len([col for col in engineered_features.columns if 'Payment' in col or 'Season' in col])}\")\n",
        "        print(f\"ðŸ‘¤ Agent Information Features: {len(agent_features)}\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Feature engineering failed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYPzl48wz-8F",
        "outputId": "aca4b713-78d1-4db0-bf19-0619b14814fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "COMPREHENSIVE FEATURE ENGINEERING PIPELINE\n",
            "============================================================\n",
            "ðŸ”„ Applying feature engineering steps...\n",
            "ðŸ”§ Processing 55 numeric columns...\n",
            "âœ… Numeric preprocessing completed: imputation + scaling\n",
            "âœ… Feature engineering completed successfully!\n",
            "ðŸ“Š Original shape: (100000, 179)\n",
            "ðŸ“ˆ Engineered shape: (100000, 42)\n",
            "ðŸŽ¯ Number of features: 42\n",
            "\n",
            "==================================================\n",
            "FEATURE CATEGORY ANALYSIS\n",
            "==================================================\n",
            "ðŸ“ Financial Risk: 5 features\n",
            "   â”œâ”€â”€ DSCR\n",
            "   â”œâ”€â”€ Loan_to_Income_Ratio\n",
            "   â”œâ”€â”€ Credit_Utilization_Ratio\n",
            "   â””â”€â”€ ... and 2 more\n",
            "ðŸ“ Customer Segmentation: 6 features\n",
            "   â”œâ”€â”€ Recency_Score\n",
            "   â”œâ”€â”€ Frequency_Score\n",
            "   â”œâ”€â”€ Monetary_Score\n",
            "   â””â”€â”€ ... and 3 more\n",
            "ðŸ“ Channel Effectiveness: 5 features\n",
            "   â”œâ”€â”€ Call_Efficiency\n",
            "   â”œâ”€â”€ SMS_Efficiency\n",
            "   â”œâ”€â”€ WhatsApp_Efficiency\n",
            "   â””â”€â”€ ... and 2 more\n",
            "ðŸ“ Temporal Patterns: 4 features\n",
            "   â”œâ”€â”€ Weekend_Payment_Preference\n",
            "   â”œâ”€â”€ Payment_Quarter\n",
            "   â”œâ”€â”€ Months_Since_Last_Payment\n",
            "   â””â”€â”€ ... and 1 more\n",
            "ðŸ“ Digital Behavior: 3 features\n",
            "   â”œâ”€â”€ Digital_Engagement_Score\n",
            "   â”œâ”€â”€ Transaction_Diversity_Score\n",
            "   â”œâ”€â”€ Channel_Adaptability_Score\n",
            "ðŸ“ Agent Information: 2 features\n",
            "   â”œâ”€â”€ Last_Successful_Agent_ID\n",
            "   â”œâ”€â”€ Best_Contact_Agent_IDs\n",
            "\n",
            "ðŸŽ¯ Total engineered features: 22\n",
            "ðŸ‘¤ Agent columns preserved: 2\n",
            "   âœ… Last_Successful_Agent_ID\n",
            "   âœ… Best_Contact_Agent_IDs\n",
            "\n",
            "============================================================\n",
            "FINAL ENGINEERED FEATURES SUMMARY\n",
            "============================================================\n",
            "âœ… Final dataset shape: (100000, 42)\n",
            "âœ… Total features: 42\n",
            "\n",
            "ðŸ“Š Sample of engineered features (first 10 columns):\n",
            "    Customer_id      DSCR  Loan_to_Income_Ratio  Credit_Utilization_Ratio  \\\n",
            "0  SCB843421788  0.760830              0.130828                  0.875595   \n",
            "1  SCB998027725  0.341316              0.400337                  0.786758   \n",
            "2  SCB871158951  2.565474             -0.117417                  0.676954   \n",
            "3  SCB938686930 -6.198262              0.102103                  2.679406   \n",
            "4  SCB843983697  0.886585              0.092542                  0.692644   \n",
            "\n",
            "   Payment_Efficiency  Composite_Financial_Stress_Score  \\\n",
            "0            0.013677                         -0.182140   \n",
            "1            0.010594                          0.508602   \n",
            "2            0.058819                         -0.044975   \n",
            "3           18.339037                         -0.098962   \n",
            "4            0.020826                         -0.174568   \n",
            "\n",
            "   Spending_Behavior_Ratio  Liquidity_Score  Recency_Score  Frequency_Score  \n",
            "0                 0.947722         0.150167       3.384104         0.284321  \n",
            "1                -5.048964        -1.352275       0.730562         0.552554  \n",
            "2                 0.729643         0.497140       0.241457         1.343988  \n",
            "3                 1.178592        -0.343503       3.384104         0.729785  \n",
            "4                 0.970164         0.486424       3.384104         0.450258  \n",
            "\n",
            "==================================================\n",
            "DOMAIN EXPERT FEATURE IMPORTANCE ANALYSIS\n",
            "==================================================\n",
            "\n",
            "CRITICAL IMPORTANCE:\n",
            "  âœ… DSCR\n",
            "  âœ… Loan_to_Income_Ratio\n",
            "  âœ… Day_Past_Due\n",
            "  âœ… Payment_Efficiency\n",
            "  âœ… Composite_Financial_Stress_Score\n",
            "\n",
            "HIGH IMPORTANCE:\n",
            "  âœ… RFM_Score\n",
            "  âœ… Behavioral_Segment\n",
            "  âœ… Multi_Channel_Engagement_Score\n",
            "  âœ… Channel_Adaptability_Score\n",
            "\n",
            "MEDIUM IMPORTANCE:\n",
            "  âœ… Digital_Engagement_Score\n",
            "  âœ… Communication_Responsiveness\n",
            "  âœ… Payment_Regularity_Score\n",
            "  âœ… Transaction_Diversity_Score\n",
            "\n",
            "CONTEXTUAL IMPORTANCE:\n",
            "  âœ… Weekend_Payment_Preference\n",
            "  âœ… Festive_Season_Payment\n",
            "  âœ… Digital_vs_Traditional_Preference\n",
            "  âœ… Agent_Effectiveness_Score\n",
            "\n",
            "ðŸ’¾ Engineered features saved to 'comprehensive_engineered_features.csv'\n",
            "ðŸ’¾ Feature pipeline saved to 'comprehensive_feature_pipeline.pkl'\n",
            "\n",
            "==================================================\n",
            "FEATURE ENGINEERING REPORT\n",
            "==================================================\n",
            "ðŸ’° Financial Risk Features: 21\n",
            "ðŸ‘¥ Customer Segmentation Features: 2\n",
            "ðŸ“± Channel Effectiveness Features: 2\n",
            "ðŸ•’ Temporal Pattern Features: 6\n",
            "ðŸ‘¤ Agent Information Features: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy import stats\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SmartChannelEffectivenessCalculator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Calculate channel effectiveness using comprehensive feature set\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        print(\"ðŸ“Š Calculating channel effectiveness scores...\")\n",
        "\n",
        "        # Calculate effectiveness for each channel using multiple factors\n",
        "        for channel in self.channels:\n",
        "            effectiveness_components = []\n",
        "\n",
        "            # 1. Historical Efficiency (if available)\n",
        "            efficiency_col = f'{channel}_Efficiency'\n",
        "            if efficiency_col in X.columns:\n",
        "                effectiveness_components.append(X[efficiency_col].fillna(0))\n",
        "                print(f\"   âœ… Using {efficiency_col} for {channel}\")\n",
        "\n",
        "            # 2. Channel Adaptability\n",
        "            if 'Channel_Adaptability_Score' in X.columns:\n",
        "                adaptability_boost = X['Channel_Adaptability_Score'] * 0.2\n",
        "                effectiveness_components.append(adaptability_boost)\n",
        "\n",
        "            # 3. Digital Preference (for digital channels)\n",
        "            if channel in ['SMS', 'WhatsApp', 'Email'] and 'Digital_vs_Traditional_Preference' in X.columns:\n",
        "                digital_boost = X['Digital_vs_Traditional_Preference'] * 0.3\n",
        "                effectiveness_components.append(digital_boost)\n",
        "\n",
        "            # 4. Traditional Preference (for traditional channels)\n",
        "            if channel in ['Call', 'IVR', 'Field_Agent'] and 'Digital_vs_Traditional_Preference' in X.columns:\n",
        "                traditional_boost = -X['Digital_vs_Traditional_Preference'] * 0.3\n",
        "                effectiveness_components.append(traditional_boost)\n",
        "\n",
        "            # 5. Customer Segment-based adjustments\n",
        "            if 'Behavioral_Segment' in X.columns:\n",
        "                segment_boost = self._get_segment_boost(X['Behavioral_Segment'], channel)\n",
        "                effectiveness_components.append(segment_boost)\n",
        "\n",
        "            # 6. Time-based adjustments\n",
        "            if 'Time_Adjusted_Responsiveness' in X.columns:\n",
        "                time_factor = X['Time_Adjusted_Responsiveness'] * 0.1\n",
        "                effectiveness_components.append(time_factor)\n",
        "\n",
        "            # Combine all components\n",
        "            if effectiveness_components:\n",
        "                # Weighted combination (historical efficiency gets highest weight)\n",
        "                if len(effectiveness_components) > 1:\n",
        "                    weights = [0.4] + [0.6/(len(effectiveness_components)-1)] * (len(effectiveness_components)-1)\n",
        "                    weighted_components = [comp * weight for comp, weight in zip(effectiveness_components, weights)]\n",
        "                    X[f'{channel}_Effectiveness_Score'] = pd.concat(weighted_components, axis=1).sum(axis=1)\n",
        "                else:\n",
        "                    X[f'{channel}_Effectiveness_Score'] = effectiveness_components[0]\n",
        "\n",
        "                # Normalize to 0-1 range\n",
        "                min_score = X[f'{channel}_Effectiveness_Score'].min()\n",
        "                max_score = X[f'{channel}_Effectiveness_Score'].max()\n",
        "                if max_score > min_score:\n",
        "                    X[f'{channel}_Effectiveness_Score'] = (X[f'{channel}_Effectiveness_Score'] - min_score) / (max_score - min_score)\n",
        "            else:\n",
        "                # Default score if no components available\n",
        "                X[f'{channel}_Effectiveness_Score'] = 0.5\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _get_segment_boost(self, segments, channel):\n",
        "        \"\"\"Get channel preference boost based on customer segment\"\"\"\n",
        "        boost = pd.Series(0, index=segments.index)\n",
        "\n",
        "        # Segment-specific channel preferences\n",
        "        segment_rules = {\n",
        "            'Digital_Savvy': {'SMS': 0.3, 'WhatsApp': 0.4, 'Email': 0.3, 'Call': -0.2, 'IVR': -0.3, 'Field_Agent': -0.4},\n",
        "            'Traditional': {'Call': 0.3, 'IVR': 0.2, 'Field_Agent': 0.3, 'SMS': -0.2, 'WhatsApp': -0.3, 'Email': -0.2},\n",
        "            'High_Risk_Delinquent': {'Call': 0.4, 'Field_Agent': 0.4, 'SMS': 0.1, 'WhatsApp': 0.1, 'Email': -0.1, 'IVR': -0.2},\n",
        "            'High_Value': {'Call': 0.3, 'Email': 0.2, 'WhatsApp': 0.2, 'SMS': 0.1, 'IVR': -0.1, 'Field_Agent': 0.3},\n",
        "            'Stable_Payer': {'SMS': 0.2, 'Email': 0.2, 'WhatsApp': 0.2, 'Call': 0.1, 'IVR': 0.1, 'Field_Agent': -0.1}\n",
        "        }\n",
        "\n",
        "        for segment, rules in segment_rules.items():\n",
        "            segment_mask = segments == segment\n",
        "            if channel in rules:\n",
        "                boost[segment_mask] = rules[channel]\n",
        "\n",
        "        return boost\n",
        "\n",
        "class FinancialContextAwareRanker(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Rank channels considering financial context and risk factors\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        self.loan_amount_threshold = None\n",
        "        self.contact_attempts_threshold = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Pre-calculate thresholds for the entire dataset\n",
        "        if 'Loan_Amount_SGD' in X.columns:\n",
        "            self.loan_amount_threshold = X['Loan_Amount_SGD'].quantile(0.75)\n",
        "\n",
        "        if 'Contact_History_Call_Attempts' in X.columns:\n",
        "            self.contact_attempts_threshold = X['Contact_History_Call_Attempts'].quantile(0.95)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        print(\"ðŸŽ¯ Ranking channels with financial context...\")\n",
        "\n",
        "        # For each customer, rank channels considering financial context\n",
        "        preference_orders = []\n",
        "        top_channels = []\n",
        "        ranking_scores = []\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            customer_scores = {}\n",
        "            row = X.iloc[i]\n",
        "\n",
        "            # Base effectiveness scores\n",
        "            for channel in self.channels:\n",
        "                score_col = f'{channel}_Effectiveness_Score'\n",
        "                if score_col in X.columns:\n",
        "                    base_score = row[score_col]\n",
        "                else:\n",
        "                    base_score = 0.5\n",
        "\n",
        "                # Financial context adjustments\n",
        "                financial_adjustment = self._calculate_financial_adjustment(row, channel)\n",
        "\n",
        "                # Risk-based adjustments\n",
        "                risk_adjustment = self._calculate_risk_adjustment(row, channel)\n",
        "\n",
        "                # Final score with adjustments\n",
        "                final_score = base_score + financial_adjustment + risk_adjustment\n",
        "                customer_scores[channel] = max(0, min(1, final_score))  # Clip to 0-1\n",
        "\n",
        "            # Sort channels by score (highest to lowest)\n",
        "            ranked_channels = sorted(customer_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Create preference order\n",
        "            preference_order = ','.join([channel for channel, score in ranked_channels])\n",
        "            preference_orders.append(preference_order)\n",
        "            top_channels.append(ranked_channels[0][0])\n",
        "            ranking_scores.append(customer_scores)\n",
        "\n",
        "        X['Channel_Preference_Order'] = preference_orders\n",
        "        X['Top_Channel'] = top_channels\n",
        "        X['Ranking_Scores'] = ranking_scores\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _calculate_financial_adjustment(self, row, channel):\n",
        "        \"\"\"Adjust scores based on financial context\"\"\"\n",
        "        adjustment = 0\n",
        "\n",
        "        # High risk customers need more personal contact\n",
        "        if 'Composite_Financial_Stress_Score' in row and row['Composite_Financial_Stress_Score'] > 0.7:\n",
        "            if channel in ['Call', 'Field_Agent']:\n",
        "                adjustment += 0.3\n",
        "            elif channel in ['SMS', 'Email']:\n",
        "                adjustment -= 0.2\n",
        "\n",
        "        # High value customers prefer professional channels\n",
        "        if 'Loan_Amount_SGD' in row and self.loan_amount_threshold is not None:\n",
        "            if row['Loan_Amount_SGD'] > self.loan_amount_threshold:\n",
        "                if channel in ['Call', 'Field_Agent', 'Email']:\n",
        "                    adjustment += 0.2\n",
        "                elif channel == 'IVR':\n",
        "                    adjustment -= 0.3\n",
        "\n",
        "        # Severe delinquency needs urgent personal contact\n",
        "        if 'Day_Past_Due' in row and row['Day_Past_Due'] > 30:\n",
        "            if channel in ['Call', 'Field_Agent']:\n",
        "                adjustment += 0.4\n",
        "            elif channel in ['SMS', 'Email', 'IVR']:\n",
        "                adjustment -= 0.3\n",
        "\n",
        "        return adjustment\n",
        "\n",
        "    def _calculate_risk_adjustment(self, row, channel):\n",
        "        \"\"\"Adjust scores based on risk factors\"\"\"\n",
        "        adjustment = 0\n",
        "\n",
        "        # Digital engagement affects digital channel preference\n",
        "        if 'Digital_Engagement_Score' in row:\n",
        "            digital_engagement = row['Digital_Engagement_Score']\n",
        "            if channel in ['SMS', 'WhatsApp', 'Email']:\n",
        "                adjustment += digital_engagement * 0.2\n",
        "            else:\n",
        "                adjustment -= digital_engagement * 0.1\n",
        "\n",
        "        # Contact history affects channel choice\n",
        "        if 'Contact_History_Call_Attempts' in row and self.contact_attempts_threshold is not None:\n",
        "            if row['Contact_History_Call_Attempts'] > self.contact_attempts_threshold:\n",
        "                if channel in ['Call', 'Field_Agent']:  # Try different approaches for hard-to-reach\n",
        "                    adjustment += 0.2\n",
        "                elif channel in ['WhatsApp', 'SMS']:  # Alternative channels\n",
        "                    adjustment += 0.1\n",
        "\n",
        "        return adjustment\n",
        "\n",
        "class MultiLevelLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Create multiple types of labels for different modeling approaches\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.cluster_model = KMeans(n_clusters=5, random_state=42)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if 'Channel_Preference_Order' in X.columns:\n",
        "            self.unique_orders = X['Channel_Preference_Order'].unique()\n",
        "            self.label_encoder.fit(self.unique_orders)\n",
        "\n",
        "        # Fit clustering for preference patterns\n",
        "        if all(f'{channel}_Effectiveness_Score' in X.columns for channel in self.channels):\n",
        "            cluster_features = X[[f'{channel}_Effectiveness_Score' for channel in self.channels]]\n",
        "            self.cluster_model.fit(cluster_features)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        print(\"ðŸ·ï¸ Creating multiple label types...\")\n",
        "\n",
        "        if 'Channel_Preference_Order' not in X.columns:\n",
        "            print(\"âŒ No preference orders found. Run FinancialContextAwareRanker first.\")\n",
        "            return X\n",
        "\n",
        "        # 1. Primary Channel Label (Multi-class)\n",
        "        X['Primary_Channel_Label'] = self.label_encoder.transform(X['Channel_Preference_Order'])\n",
        "\n",
        "        # 2. Top Channel Label (Simplified)\n",
        "        X['Top_Channel_Label'] = X['Top_Channel']\n",
        "\n",
        "        # 3. Top-3 Channels (Multi-label)\n",
        "        for channel in self.channels:\n",
        "            X[f'Prefers_{channel}_Top3'] = X['Channel_Preference_Order'].apply(\n",
        "                lambda x: 1 if channel in x.split(',')[:3] else 0\n",
        "            )\n",
        "\n",
        "        # 4. Channel Preference Cluster\n",
        "        if all(f'{channel}_Effectiveness_Score' in X.columns for channel in self.channels):\n",
        "            cluster_features = X[[f'{channel}_Effectiveness_Score' for channel in self.channels]]\n",
        "            X['Preference_Cluster'] = self.cluster_model.predict(cluster_features)\n",
        "\n",
        "        # 5. Urgency-based Channel Label (High/Medium/Low Touch)\n",
        "        X['Contact_Urgency_Level'] = X.apply(self._calculate_urgency_level, axis=1)\n",
        "\n",
        "        # 6. Binary labels for key channels\n",
        "        X['Prefers_Personal_Contact'] = X['Channel_Preference_Order'].apply(\n",
        "            lambda x: 1 if any(channel in x.split(',')[0] for channel in ['Call', 'Field_Agent']) else 0\n",
        "        )\n",
        "\n",
        "        X['Prefers_Digital_Contact'] = X['Channel_Preference_Order'].apply(\n",
        "            lambda x: 1 if any(channel in x.split(',')[0] for channel in ['SMS', 'WhatsApp', 'Email']) else 0\n",
        "        )\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _calculate_urgency_level(self, row):\n",
        "        \"\"\"Calculate contact urgency level based on financial risk\"\"\"\n",
        "        urgency_score = 0\n",
        "\n",
        "        # Financial stress increases urgency\n",
        "        if 'Composite_Financial_Stress_Score' in row:\n",
        "            urgency_score += row['Composite_Financial_Stress_Score'] * 2\n",
        "\n",
        "        # Delinquency increases urgency\n",
        "        if 'Day_Past_Due' in row and row['Day_Past_Due'] > 30:\n",
        "            urgency_score += 1\n",
        "\n",
        "        # High risk indicators increase urgency\n",
        "        if 'Financial_Stress_Score' in row and row['Financial_Stress_Score'] > 0.7:\n",
        "            urgency_score += 1\n",
        "\n",
        "        # Categorize urgency\n",
        "        if urgency_score >= 3:\n",
        "            return 'High_Urgency'\n",
        "        elif urgency_score >= 1.5:\n",
        "            return 'Medium_Urgency'\n",
        "        else:\n",
        "            return 'Low_Urgency'\n",
        "\n",
        "class AdvancedLabelAnalyzer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Analyze and validate the created labels\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.analysis_results = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ“Š ADVANCED LABEL ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self._analyze_channel_distribution(X)\n",
        "        self._analyze_segment_preferences(X)\n",
        "        self._analyze_financial_context_patterns(X)\n",
        "        self._analyze_label_quality(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _analyze_channel_distribution(self, X):\n",
        "        \"\"\"Analyze distribution of channel preferences\"\"\"\n",
        "        print(\"\\nðŸŽ¯ CHANNEL PREFERENCE DISTRIBUTION:\")\n",
        "\n",
        "        # Top channel distribution\n",
        "        if 'Top_Channel' in X.columns:\n",
        "            top_dist = X['Top_Channel'].value_counts()\n",
        "            print(\"\\nðŸ† Top Channel Preferences:\")\n",
        "            for channel, count in top_dist.items():\n",
        "                percentage = (count / len(X)) * 100\n",
        "                print(f\"   {channel}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "        # Top-3 channel presence\n",
        "        if any(f'Prefers_{channel}_Top3' in X.columns for channel in ['Call', 'SMS', 'WhatsApp', 'Email']):\n",
        "            print(\"\\nðŸ“± Channel Presence in Top 3:\")\n",
        "            for channel in ['Call', 'SMS', 'WhatsApp', 'Email']:\n",
        "                col = f'Prefers_{channel}_Top3'\n",
        "                if col in X.columns:\n",
        "                    count = X[col].sum()\n",
        "                    percentage = (count / len(X)) * 100\n",
        "                    print(f\"   {channel}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "    def _analyze_segment_preferences(self, X):\n",
        "        \"\"\"Analyze channel preferences by customer segment\"\"\"\n",
        "        if 'Behavioral_Segment' in X.columns and 'Top_Channel' in X.columns:\n",
        "            print(\"\\nðŸ‘¥ CHANNEL PREFERENCES BY SEGMENT:\")\n",
        "\n",
        "            segment_channel_pref = X.groupby('Behavioral_Segment')['Top_Channel'].agg(\n",
        "                lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
        "            )\n",
        "\n",
        "            for segment, preferred_channel in segment_channel_pref.items():\n",
        "                segment_count = (X['Behavioral_Segment'] == segment).sum()\n",
        "                print(f\"   {segment}: Prefers {preferred_channel} ({segment_count} customers)\")\n",
        "\n",
        "    def _analyze_financial_context_patterns(self, X):\n",
        "        \"\"\"Analyze how financial context affects channel preferences\"\"\"\n",
        "        print(\"\\nðŸ’° FINANCIAL CONTEXT PATTERNS:\")\n",
        "\n",
        "        # High risk customers' preferences\n",
        "        if 'Composite_Financial_Stress_Score' in X.columns and 'Top_Channel' in X.columns:\n",
        "            high_risk_mask = X['Composite_Financial_Stress_Score'] > 0.7\n",
        "            if high_risk_mask.any():\n",
        "                high_risk_pref = X[high_risk_mask]['Top_Channel'].value_counts()\n",
        "                print(\"   High-Risk Customers prefer:\")\n",
        "                for channel, count in high_risk_pref.items():\n",
        "                    percentage = (count / high_risk_mask.sum()) * 100\n",
        "                    print(f\"     {channel}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        # High value customers' preferences\n",
        "        if 'Loan_Amount_SGD' in X.columns and 'Top_Channel' in X.columns:\n",
        "            high_value_threshold = X['Loan_Amount_SGD'].quantile(0.75)\n",
        "            high_value_mask = X['Loan_Amount_SGD'] > high_value_threshold\n",
        "            if high_value_mask.any():\n",
        "                high_value_pref = X[high_value_mask]['Top_Channel'].value_counts()\n",
        "                print(\"   High-Value Customers prefer:\")\n",
        "                for channel, count in high_value_pref.items():\n",
        "                    percentage = (count / high_value_mask.sum()) * 100\n",
        "                    print(f\"     {channel}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def _analyze_label_quality(self, X):\n",
        "        \"\"\"Analyze the quality and diversity of created labels\"\"\"\n",
        "        print(\"\\nðŸ“ˆ LABEL QUALITY METRICS:\")\n",
        "\n",
        "        # Label diversity\n",
        "        if 'Channel_Preference_Order' in X.columns:\n",
        "            unique_orders = X['Channel_Preference_Order'].nunique()\n",
        "            total_customers = len(X)\n",
        "            diversity_ratio = unique_orders / total_customers\n",
        "\n",
        "            print(f\"   Unique preference patterns: {unique_orders}\")\n",
        "            print(f\"   Diversity ratio: {diversity_ratio:.3f}\")\n",
        "\n",
        "            # Show most common patterns\n",
        "            common_patterns = X['Channel_Preference_Order'].value_counts().head(5)\n",
        "            print(\"\\n   Most common preference patterns:\")\n",
        "            for pattern, count in common_patterns.items():\n",
        "                percentage = (count / total_customers) * 100\n",
        "                print(f\"     {pattern}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "class BestChannelRankingLabeler(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Create the final best channel ranking labels for model training\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        self.best_channel_encoder = LabelEncoder()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if 'Top_Channel' in X.columns:\n",
        "            self.best_channel_encoder.fit(X['Top_Channel'])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        print(\"ðŸ† Creating final best channel ranking labels...\")\n",
        "\n",
        "        # 1. Primary target: Best Channel (Multi-class classification)\n",
        "        X['Best_Channel_Label'] = self.best_channel_encoder.transform(X['Top_Channel'])\n",
        "\n",
        "        # 2. Channel Ranking Scores (Regression targets)\n",
        "        for channel in self.channels:\n",
        "            score_col = f'{channel}_Effectiveness_Score'\n",
        "            if score_col in X.columns:\n",
        "                X[f'{channel}_Ranking_Score'] = X[score_col]\n",
        "\n",
        "        # 3. Preference Strength Indicator\n",
        "        if 'Ranking_Scores' in X.columns:\n",
        "            X['Preference_Strength'] = X['Ranking_Scores'].apply(\n",
        "                lambda scores: max(scores.values()) - min(scores.values()) if scores else 0\n",
        "            )\n",
        "\n",
        "        # 4. Channel Switch Recommendation\n",
        "        X['Recommended_Channel_Switch'] = X.apply(self._recommend_channel_switch, axis=1)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _recommend_channel_switch(self, row):\n",
        "        \"\"\"Recommend if customer should switch from current top channel\"\"\"\n",
        "        if 'Top_Channel' not in row or 'Ranking_Scores' not in row:\n",
        "            return 'No_Recommendation'\n",
        "\n",
        "        current_top = row['Top_Channel']\n",
        "        scores = row['Ranking_Scores']\n",
        "\n",
        "        if not scores or current_top not in scores:\n",
        "            return 'No_Recommendation'\n",
        "\n",
        "        current_score = scores[current_top]\n",
        "        max_score = max(scores.values())\n",
        "\n",
        "        # Recommend switch if there's a significantly better channel\n",
        "        if max_score - current_score > 0.2:\n",
        "            best_channel = max(scores.items(), key=lambda x: x[1])[0]\n",
        "            return f'Switch_to_{best_channel}'\n",
        "        else:\n",
        "            return 'Maintain_Current'\n",
        "\n",
        "class IntelligentLabelingPipeline:\n",
        "    \"\"\"Complete pipeline for creating intelligent channel preference labels\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.labels_df = None\n",
        "        self._build_pipeline()\n",
        "\n",
        "    def _build_pipeline(self):\n",
        "        \"\"\"Build the intelligent labeling pipeline\"\"\"\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            # Step 1: Calculate comprehensive channel effectiveness\n",
        "            ('effectiveness_calculator', SmartChannelEffectivenessCalculator()),\n",
        "\n",
        "            # Step 2: Rank channels with financial context\n",
        "            ('channel_ranker', FinancialContextAwareRanker()),\n",
        "\n",
        "            # Step 3: Create multiple label types\n",
        "            ('label_encoder', MultiLevelLabelEncoder()),\n",
        "\n",
        "            # Step 4: Create final best channel ranking labels\n",
        "            ('best_channel_labeler', BestChannelRankingLabeler()),\n",
        "\n",
        "            # Step 5: Analyze and validate labels\n",
        "            ('label_analyzer', AdvancedLabelAnalyzer())\n",
        "        ])\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Create intelligent labels from engineered features\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"ðŸ¤– INTELLIGENT CHANNEL PREFERENCE LABELING\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Store original Customer_id and Agent IDs\n",
        "            original_columns = {}\n",
        "            important_columns = ['Customer_id', 'Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "\n",
        "            for col in important_columns:\n",
        "                if col in X.columns:\n",
        "                    original_columns[col] = X[col].copy()\n",
        "\n",
        "            # Transform data\n",
        "            print(\"ðŸ”„ Applying feature engineering steps...\")\n",
        "            X_transformed = self.pipeline.fit_transform(X)\n",
        "\n",
        "            # Ensure important columns are preserved\n",
        "            for col, values in original_columns.items():\n",
        "                if col not in X_transformed.columns:\n",
        "                    X_transformed[col] = values\n",
        "\n",
        "            # Move Customer_id to first column\n",
        "            if 'Customer_id' in X_transformed.columns:\n",
        "                cols = ['Customer_id'] + [col for col in X_transformed.columns if col != 'Customer_id']\n",
        "                X_transformed = X_transformed[cols]\n",
        "\n",
        "            # Extract label columns for easy access\n",
        "            label_columns = self._get_label_columns(X_transformed)\n",
        "            self.labels_df = X_transformed[label_columns]\n",
        "\n",
        "            print(\"\\nâœ… Label construction completed successfully!\")\n",
        "            print(f\"ðŸ“Š Number of customers labeled: {len(self.labels_df)}\")\n",
        "            print(f\"ðŸŽ¯ Number of unique preference patterns: {self.labels_df['Channel_Preference_Order'].nunique()}\")\n",
        "\n",
        "            # Show target variable distribution\n",
        "            self._show_target_distribution()\n",
        "\n",
        "            return X_transformed, self.labels_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in label construction: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return X, None\n",
        "\n",
        "    def _get_label_columns(self, X):\n",
        "        \"\"\"Get all label-related columns\"\"\"\n",
        "        label_columns = ['Customer_id', 'Channel_Preference_Order', 'Top_Channel',\n",
        "                       'Primary_Channel_Label', 'Top_Channel_Label', 'Best_Channel_Label',\n",
        "                       'Preference_Cluster', 'Contact_Urgency_Level',\n",
        "                       'Prefers_Personal_Contact', 'Prefers_Digital_Contact',\n",
        "                       'Preference_Strength', 'Recommended_Channel_Switch']\n",
        "\n",
        "        # Add binary preference columns\n",
        "        binary_cols = [col for col in X.columns if 'Prefers_' in col and 'Top3' in col]\n",
        "        label_columns.extend(binary_cols)\n",
        "\n",
        "        # Add ranking score columns\n",
        "        ranking_cols = [col for col in X.columns if 'Ranking_Score' in col]\n",
        "        label_columns.extend(ranking_cols)\n",
        "\n",
        "        # Add agent columns if they exist\n",
        "        agent_cols = [col for col in ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "                     if col in X.columns]\n",
        "        label_columns.extend(agent_cols)\n",
        "\n",
        "        return [col for col in label_columns if col in X.columns]\n",
        "\n",
        "    def _show_target_distribution(self):\n",
        "        \"\"\"Show distribution of target variables\"\"\"\n",
        "        if self.labels_df is None:\n",
        "            return\n",
        "\n",
        "        print(\"\\nðŸŽ¯ TARGET VARIABLE DISTRIBUTION:\")\n",
        "\n",
        "        # Best Channel Distribution\n",
        "        if 'Best_Channel_Label' in self.labels_df.columns:\n",
        "            channel_dist = self.labels_df['Top_Channel'].value_counts()\n",
        "            print(\"\\nðŸ† Best Channel Distribution:\")\n",
        "            for channel, count in channel_dist.items():\n",
        "                percentage = (count / len(self.labels_df)) * 100\n",
        "                print(f\"   {channel}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "        # Urgency Level Distribution\n",
        "        if 'Contact_Urgency_Level' in self.labels_df.columns:\n",
        "            urgency_dist = self.labels_df['Contact_Urgency_Level'].value_counts()\n",
        "            print(\"\\nðŸš¨ Contact Urgency Distribution:\")\n",
        "            for level, count in urgency_dist.items():\n",
        "                percentage = (count / len(self.labels_df)) * 100\n",
        "                print(f\"   {level}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "    def get_label_summary(self):\n",
        "        \"\"\"Get comprehensive summary of created labels\"\"\"\n",
        "        if self.labels_df is None:\n",
        "            print(\"No labels available. Run fit_transform first.\")\n",
        "            return\n",
        "\n",
        "        summary = {\n",
        "            'total_customers': len(self.labels_df),\n",
        "            'unique_preference_patterns': self.labels_df['Channel_Preference_Order'].nunique(),\n",
        "            'top_channel_distribution': self.labels_df['Top_Channel'].value_counts().to_dict(),\n",
        "            'best_channel_labels': self.labels_df['Best_Channel_Label'].value_counts().to_dict() if 'Best_Channel_Label' in self.labels_df.columns else {},\n",
        "            'urgency_level_distribution': self.labels_df['Contact_Urgency_Level'].value_counts().to_dict() if 'Contact_Urgency_Level' in self.labels_df.columns else {},\n",
        "            'preference_clusters': self.labels_df['Preference_Cluster'].value_counts().to_dict() if 'Preference_Cluster' in self.labels_df.columns else {}\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def save_labels(self, filepath='best_channel_ranking_labels.csv'):\n",
        "        \"\"\"Save the labels to CSV\"\"\"\n",
        "        if self.labels_df is not None:\n",
        "            self.labels_df.to_csv(filepath, index=False)\n",
        "            print(f\"ðŸ’¾ Labels saved to {filepath}\")\n",
        "        else:\n",
        "            print(\"âŒ No labels to save\")\n",
        "\n",
        "    def get_training_data(self, features_df):\n",
        "        \"\"\"Prepare features and targets for model training\"\"\"\n",
        "        if self.labels_df is None:\n",
        "            print(\"No labels available. Run fit_transform first.\")\n",
        "            return None, None\n",
        "\n",
        "        # Merge features with labels\n",
        "        training_data = features_df.merge(\n",
        "            self.labels_df[['Customer_id', 'Best_Channel_Label', 'Top_Channel']],\n",
        "            on='Customer_id',\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        # Separate features and target\n",
        "        feature_columns = [col for col in training_data.columns if col not in\n",
        "                         ['Customer_id', 'Best_Channel_Label', 'Top_Channel',\n",
        "                          'Channel_Preference_Order', 'Ranking_Scores']]\n",
        "\n",
        "        X = training_data[feature_columns]\n",
        "        y = training_data['Best_Channel_Label']\n",
        "\n",
        "        print(f\"ðŸ“š Training data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "        print(f\"ðŸŽ¯ Target variable: Best_Channel_Label ({len(y.unique())} classes)\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "# Quick usage function\n",
        "def create_best_channel_labels(engineered_features_df):\n",
        "    \"\"\"\n",
        "    One-function approach to create best channel ranking labels\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    engineered_features_df : pandas.DataFrame\n",
        "        Output from ComprehensiveFeaturePipeline\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (features_with_labels, labels_dataframe, training_data)\n",
        "    \"\"\"\n",
        "    # Initialize pipeline\n",
        "    label_pipeline = IntelligentLabelingPipeline()\n",
        "\n",
        "    # Create labels\n",
        "    features_with_labels, labels_df = label_pipeline.fit_transform(engineered_features_df)\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train, y_train = None, None\n",
        "    if labels_df is not None:\n",
        "        X_train, y_train = label_pipeline.get_training_data(engineered_features_df)\n",
        "\n",
        "        # Save results\n",
        "        label_pipeline.save_labels('best_channel_ranking_labels.csv')\n",
        "        features_with_labels.to_csv('features_with_best_channel_labels.csv', index=False)\n",
        "\n",
        "        # Print summary\n",
        "        summary = label_pipeline.get_label_summary()\n",
        "        print(\"\\nðŸ“‹ LABELING SUMMARY:\")\n",
        "        print(f\"   Total customers: {summary['total_customers']}\")\n",
        "        print(f\"   Unique patterns: {summary['unique_preference_patterns']}\")\n",
        "        print(f\"   Most common best channel: {max(summary['top_channel_distribution'].items(), key=lambda x: x[1])[0]}\")\n",
        "        print(f\"   Training samples: {X_train.shape[0] if X_train is not None else 0}\")\n",
        "        print(f\"   Features for training: {X_train.shape[1] if X_train is not None else 0}\")\n",
        "\n",
        "    return features_with_labels, labels_df, (X_train, y_train)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your engineered features\n",
        "    print(\"ðŸ” Loading engineered features...\")\n",
        "    df = pd.read_csv('comprehensive_engineered_features.csv')\n",
        "\n",
        "    print(f\"ðŸ“Š Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "    print(f\"ðŸ‘¤ Customer count: {df['Customer_id'].nunique() if 'Customer_id' in df.columns else 'N/A'}\")\n",
        "\n",
        "    # Create intelligent labels\n",
        "    features_with_labels, labels, training_data = create_best_channel_labels(df)\n",
        "\n",
        "    if labels is not None:\n",
        "        print(\"\\nðŸŽ‰ SUCCESS! Best channel ranking labels created.\")\n",
        "        print(\"\\nðŸ“‹ Sample of created labels:\")\n",
        "        sample_columns = ['Customer_id', 'Top_Channel', 'Best_Channel_Label', 'Contact_Urgency_Level']\n",
        "        available_columns = [col for col in sample_columns if col in labels.columns]\n",
        "        print(labels[available_columns].head(10))\n",
        "\n",
        "        # Show different label types available for modeling\n",
        "        print(\"\\nðŸ·ï¸ AVAILABLE TARGET VARIABLES FOR MODELING:\")\n",
        "        target_variables = {\n",
        "            'Multi-class Classification': ['Best_Channel_Label', 'Top_Channel_Label'],\n",
        "            'Multi-label Classification': [col for col in labels.columns if 'Prefers_' in col and 'Top3' in col],\n",
        "            'Regression': [col for col in labels.columns if 'Ranking_Score' in col],\n",
        "            'Binary Classification': ['Prefers_Personal_Contact', 'Prefers_Digital_Contact'],\n",
        "            'Clustering': ['Preference_Cluster'],\n",
        "            'Strategic Segmentation': ['Contact_Urgency_Level', 'Recommended_Channel_Switch']\n",
        "        }\n",
        "\n",
        "        for model_type, expected_cols in target_variables.items():\n",
        "            available_cols = [col for col in expected_cols if col in labels.columns]\n",
        "            if available_cols:\n",
        "                print(f\"   {model_type}: {', '.join(available_cols)}\")\n",
        "\n",
        "        # Show training data info\n",
        "        if training_data[0] is not None:\n",
        "            print(f\"\\nðŸ“š TRAINING DATA READY:\")\n",
        "            print(f\"   Features shape: {training_data[0].shape}\")\n",
        "            print(f\"   Target shape: {training_data[1].shape}\")\n",
        "            print(f\"   Unique target classes: {len(np.unique(training_data[1]))}\")\n",
        "    else:\n",
        "        print(\"âŒ Label creation failed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmBQjPw-EITP",
        "outputId": "3d1bbd2e-7ece-4148-e748-7845bde22cd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Loading engineered features...\n",
            "ðŸ“Š Loaded data: 100000 rows, 42 columns\n",
            "ðŸ‘¤ Customer count: 100000\n",
            "============================================================\n",
            "ðŸ¤– INTELLIGENT CHANNEL PREFERENCE LABELING\n",
            "============================================================\n",
            "ðŸ”„ Applying feature engineering steps...\n",
            "ðŸ“Š Calculating channel effectiveness scores...\n",
            "   âœ… Using Call_Efficiency for Call\n",
            "   âœ… Using SMS_Efficiency for SMS\n",
            "   âœ… Using WhatsApp_Efficiency for WhatsApp\n",
            "ðŸŽ¯ Ranking channels with financial context...\n",
            "ðŸ·ï¸ Creating multiple label types...\n",
            "ðŸ† Creating final best channel ranking labels...\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š ADVANCED LABEL ANALYSIS\n",
            "============================================================\n",
            "\n",
            "ðŸŽ¯ CHANNEL PREFERENCE DISTRIBUTION:\n",
            "\n",
            "ðŸ† Top Channel Preferences:\n",
            "   Call: 40463 customers (40.5%)\n",
            "   Email: 21365 customers (21.4%)\n",
            "   Field_Agent: 14552 customers (14.6%)\n",
            "   SMS: 14334 customers (14.3%)\n",
            "   WhatsApp: 9123 customers (9.1%)\n",
            "   IVR: 163 customers (0.2%)\n",
            "\n",
            "ðŸ“± Channel Presence in Top 3:\n",
            "   Call: 65143 customers (65.1%)\n",
            "   SMS: 35537 customers (35.5%)\n",
            "   WhatsApp: 37823 customers (37.8%)\n",
            "   Email: 58090 customers (58.1%)\n",
            "\n",
            "ðŸ‘¥ CHANNEL PREFERENCES BY SEGMENT:\n",
            "   Digital_Savvy: Prefers Email (4882 customers)\n",
            "   High_Value: Prefers Email (3416 customers)\n",
            "   Stable_Payer: Prefers Email (3104 customers)\n",
            "   Standard: Prefers Email (48788 customers)\n",
            "   Traditional: Prefers Call (39810 customers)\n",
            "\n",
            "ðŸ’° FINANCIAL CONTEXT PATTERNS:\n",
            "   High-Risk Customers prefer:\n",
            "     Call: 5521 (81.1%)\n",
            "     Field_Agent: 1185 (17.4%)\n",
            "     WhatsApp: 103 (1.5%)\n",
            "     Email: 1 (0.0%)\n",
            "   High-Value Customers prefer:\n",
            "     Email: 10633 (43.2%)\n",
            "     Call: 10419 (42.3%)\n",
            "     Field_Agent: 3568 (14.5%)\n",
            "     WhatsApp: 1 (0.0%)\n",
            "\n",
            "ðŸ“ˆ LABEL QUALITY METRICS:\n",
            "   Unique preference patterns: 276\n",
            "   Diversity ratio: 0.003\n",
            "\n",
            "   Most common preference patterns:\n",
            "     Call,Field_Agent,IVR,SMS,Email,WhatsApp: 11224 customers (11.2%)\n",
            "     SMS,Email,WhatsApp,Call,Field_Agent,IVR: 8409 customers (8.4%)\n",
            "     Call,Field_Agent,IVR,Email,WhatsApp,SMS: 5005 customers (5.0%)\n",
            "     Call,Field_Agent,Email,SMS,WhatsApp,IVR: 4582 customers (4.6%)\n",
            "     Call,Field_Agent,Email,WhatsApp,SMS,IVR: 4545 customers (4.5%)\n",
            "\n",
            "âœ… Label construction completed successfully!\n",
            "ðŸ“Š Number of customers labeled: 100000\n",
            "ðŸŽ¯ Number of unique preference patterns: 276\n",
            "\n",
            "ðŸŽ¯ TARGET VARIABLE DISTRIBUTION:\n",
            "\n",
            "ðŸ† Best Channel Distribution:\n",
            "   Call: 40463 customers (40.5%)\n",
            "   Email: 21365 customers (21.4%)\n",
            "   Field_Agent: 14552 customers (14.6%)\n",
            "   SMS: 14334 customers (14.3%)\n",
            "   WhatsApp: 9123 customers (9.1%)\n",
            "   IVR: 163 customers (0.2%)\n",
            "\n",
            "ðŸš¨ Contact Urgency Distribution:\n",
            "   Low_Urgency: 80812 customers (80.8%)\n",
            "   Medium_Urgency: 19096 customers (19.1%)\n",
            "   High_Urgency: 92 customers (0.1%)\n",
            "ðŸ“š Training data prepared: 100000 samples, 41 features\n",
            "ðŸŽ¯ Target variable: Best_Channel_Label (6 classes)\n",
            "ðŸ’¾ Labels saved to best_channel_ranking_labels.csv\n",
            "\n",
            "ðŸ“‹ LABELING SUMMARY:\n",
            "   Total customers: 100000\n",
            "   Unique patterns: 276\n",
            "   Most common best channel: Call\n",
            "   Training samples: 100000\n",
            "   Features for training: 41\n",
            "\n",
            "ðŸŽ‰ SUCCESS! Best channel ranking labels created.\n",
            "\n",
            "ðŸ“‹ Sample of created labels:\n",
            "    Customer_id  Top_Channel  Best_Channel_Label Contact_Urgency_Level\n",
            "0  SCB843421788         Call                   0           Low_Urgency\n",
            "1  SCB998027725         Call                   0        Medium_Urgency\n",
            "2  SCB871158951          SMS                   4           Low_Urgency\n",
            "3  SCB938686930  Field_Agent                   2           Low_Urgency\n",
            "4  SCB843983697          SMS                   4           Low_Urgency\n",
            "5  SCB946928259  Field_Agent                   2           Low_Urgency\n",
            "6  SCB900671743         Call                   0        Medium_Urgency\n",
            "7  SCB882059819         Call                   0        Medium_Urgency\n",
            "8  SCB892183816  Field_Agent                   2           Low_Urgency\n",
            "9  SCB831408653     WhatsApp                   5           Low_Urgency\n",
            "\n",
            "ðŸ·ï¸ AVAILABLE TARGET VARIABLES FOR MODELING:\n",
            "   Multi-class Classification: Best_Channel_Label, Top_Channel_Label\n",
            "   Multi-label Classification: Prefers_Call_Top3, Prefers_SMS_Top3, Prefers_WhatsApp_Top3, Prefers_Email_Top3, Prefers_IVR_Top3, Prefers_Field_Agent_Top3\n",
            "   Regression: Ranking_Scores, Call_Ranking_Score, SMS_Ranking_Score, WhatsApp_Ranking_Score, Email_Ranking_Score, IVR_Ranking_Score, Field_Agent_Ranking_Score\n",
            "   Binary Classification: Prefers_Personal_Contact, Prefers_Digital_Contact\n",
            "   Clustering: Preference_Cluster\n",
            "   Strategic Segmentation: Contact_Urgency_Level, Recommended_Channel_Switch\n",
            "\n",
            "ðŸ“š TRAINING DATA READY:\n",
            "   Features shape: (100000, 41)\n",
            "   Target shape: (100000,)\n",
            "   Unique target classes: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ChannelEffectivenessCalculator(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Calculate how effective each channel is for each customer using comprehensive features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        print(\"ðŸ“Š Calculating channel effectiveness scores...\")\n",
        "\n",
        "        # For each channel, calculate an effectiveness score using multiple factors\n",
        "        for channel in self.channels:\n",
        "            effectiveness_components = []\n",
        "\n",
        "            # 1. Historical Efficiency (if available)\n",
        "            efficiency_col = f'{channel}_Efficiency'\n",
        "            if efficiency_col in X.columns:\n",
        "                effectiveness_components.append(X[efficiency_col].fillna(0))\n",
        "                print(f\"   âœ… Using {efficiency_col} for {channel}\")\n",
        "\n",
        "            # 2. Channel Adaptability\n",
        "            if 'Channel_Adaptability_Score' in X.columns:\n",
        "                adaptability_boost = X['Channel_Adaptability_Score'] * 0.2\n",
        "                effectiveness_components.append(adaptability_boost)\n",
        "\n",
        "            # 3. Digital Preference (for digital channels)\n",
        "            if channel in ['SMS', 'WhatsApp', 'Email'] and 'Digital_vs_Traditional_Preference' in X.columns:\n",
        "                digital_boost = X['Digital_vs_Traditional_Preference'] * 0.3\n",
        "                effectiveness_components.append(digital_boost)\n",
        "\n",
        "            # 4. Traditional Preference (for traditional channels)\n",
        "            if channel in ['Call', 'IVR', 'Field_Agent'] and 'Digital_vs_Traditional_Preference' in X.columns:\n",
        "                traditional_boost = -X['Digital_vs_Traditional_Preference'] * 0.3\n",
        "                effectiveness_components.append(traditional_boost)\n",
        "\n",
        "            # 5. Customer Segment-based adjustments\n",
        "            if 'Behavioral_Segment' in X.columns:\n",
        "                segment_boost = self._get_segment_boost(X['Behavioral_Segment'], channel)\n",
        "                effectiveness_components.append(segment_boost)\n",
        "\n",
        "            # 6. Time-based adjustments\n",
        "            if 'Time_Adjusted_Responsiveness' in X.columns:\n",
        "                time_factor = X['Time_Adjusted_Responsiveness'] * 0.1\n",
        "                effectiveness_components.append(time_factor)\n",
        "\n",
        "            # Combine all components\n",
        "            if effectiveness_components:\n",
        "                # Weighted combination (historical efficiency gets highest weight)\n",
        "                if len(effectiveness_components) > 1:\n",
        "                    weights = [0.4] + [0.6/(len(effectiveness_components)-1)] * (len(effectiveness_components)-1)\n",
        "                    weighted_components = [comp * weight for comp, weight in zip(effectiveness_components, weights)]\n",
        "                    effectiveness_score = pd.concat(weighted_components, axis=1).sum(axis=1)\n",
        "                else:\n",
        "                    effectiveness_score = effectiveness_components[0]\n",
        "\n",
        "                # Normalize to 0-1 range\n",
        "                min_score = effectiveness_score.min()\n",
        "                max_score = effectiveness_score.max()\n",
        "                if max_score > min_score:\n",
        "                    effectiveness_score = (effectiveness_score - min_score) / (max_score - min_score)\n",
        "\n",
        "                X[f'{channel}_Effectiveness'] = effectiveness_score\n",
        "            else:\n",
        "                # Default score if no components available\n",
        "                X[f'{channel}_Effectiveness'] = 0.5\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _get_segment_boost(self, segments, channel):\n",
        "        \"\"\"Get channel preference boost based on customer segment\"\"\"\n",
        "        boost = pd.Series(0, index=segments.index)\n",
        "\n",
        "        # Segment-specific channel preferences\n",
        "        segment_rules = {\n",
        "            'Digital_Savvy': {'SMS': 0.3, 'WhatsApp': 0.4, 'Email': 0.3, 'Call': -0.2, 'IVR': -0.3, 'Field_Agent': -0.4},\n",
        "            'Traditional': {'Call': 0.3, 'IVR': 0.2, 'Field_Agent': 0.3, 'SMS': -0.2, 'WhatsApp': -0.3, 'Email': -0.2},\n",
        "            'High_Risk_Delinquent': {'Call': 0.4, 'Field_Agent': 0.4, 'SMS': 0.1, 'WhatsApp': 0.1, 'Email': -0.1, 'IVR': -0.2},\n",
        "            'High_Value': {'Call': 0.3, 'Email': 0.2, 'WhatsApp': 0.2, 'SMS': 0.1, 'IVR': -0.1, 'Field_Agent': 0.3},\n",
        "            'Stable_Payer': {'SMS': 0.2, 'Email': 0.2, 'WhatsApp': 0.2, 'Call': 0.1, 'IVR': 0.1, 'Field_Agent': -0.1}\n",
        "        }\n",
        "\n",
        "        for segment, rules in segment_rules.items():\n",
        "            segment_mask = segments == segment\n",
        "            if channel in rules:\n",
        "                boost[segment_mask] = rules[channel]\n",
        "\n",
        "        return boost\n",
        "\n",
        "class ChannelRanker(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Rank channels from best to worst for each customer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        print(\"ðŸŽ¯ Ranking channels from best to worst...\")\n",
        "\n",
        "        # For each customer, rank channels by effectiveness\n",
        "        preference_orders = []\n",
        "        top_channels = []\n",
        "\n",
        "        for i in range(len(X)):\n",
        "            customer_scores = {}\n",
        "\n",
        "            # Get scores for this customer\n",
        "            for channel in self.channels:\n",
        "                score_col = f'{channel}_Effectiveness'\n",
        "                if score_col in X.columns:\n",
        "                    customer_scores[channel] = X[score_col].iloc[i]\n",
        "                else:\n",
        "                    customer_scores[channel] = 0  # Default if missing\n",
        "\n",
        "            # Sort channels by score (highest to lowest)\n",
        "            ranked_channels = sorted(customer_scores.items(),\n",
        "                                   key=lambda x: x[1],\n",
        "                                   reverse=True)\n",
        "\n",
        "            # Create preference order string\n",
        "            preference_order = ','.join([channel for channel, score in ranked_channels])\n",
        "            preference_orders.append(preference_order)\n",
        "            top_channels.append(ranked_channels[0][0])\n",
        "\n",
        "        # Add preference order to dataframe\n",
        "        X['Channel_Preference_Order'] = preference_orders\n",
        "        X['Top_Channel'] = top_channels\n",
        "\n",
        "        return X\n",
        "\n",
        "class PreferenceLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Convert preference orders into model-friendly formats\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.top_channel_encoder = LabelEncoder()\n",
        "        self.unique_orders = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if 'Channel_Preference_Order' in X.columns:\n",
        "            # Learn all possible preference patterns\n",
        "            self.unique_orders = X['Channel_Preference_Order'].unique()\n",
        "            self.label_encoder.fit(self.unique_orders)\n",
        "\n",
        "        if 'Top_Channel' in X.columns:\n",
        "            # Learn all possible top channels\n",
        "            self.top_channel_encoder.fit(X['Top_Channel'])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        print(\"ðŸ·ï¸ Converting preferences to model-friendly formats...\")\n",
        "\n",
        "        if 'Channel_Preference_Order' not in X.columns:\n",
        "            print(\"No preference orders found. Run ChannelRanker first.\")\n",
        "            return X\n",
        "\n",
        "        # Method 1: Encode entire order as one label (like animal classification)\n",
        "        X['Preference_Label'] = self.label_encoder.transform(X['Channel_Preference_Order'])\n",
        "\n",
        "        # Method 2: Create simple \"top choice\" label\n",
        "        X['Top_Channel_Label'] = self.top_channel_encoder.transform(X['Top_Channel'])\n",
        "\n",
        "        # Method 3: Binary indicators for each channel being in top 3\n",
        "        for channel in ['Call', 'SMS', 'WhatsApp', 'Email']:\n",
        "            X[f'Prefers_{channel}_Top3'] = X['Channel_Preference_Order'].apply(\n",
        "                lambda x: 1 if channel in x.split(',')[:3] else 0\n",
        "            )\n",
        "\n",
        "        return X\n",
        "\n",
        "class LabelConstructionPipeline:\n",
        "    \"\"\"Complete pipeline for creating channel preference labels\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.labels = None\n",
        "        self._build_pipeline()\n",
        "\n",
        "    def _build_pipeline(self):\n",
        "        \"\"\"Build the label construction pipeline\"\"\"\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            # Step 1: Calculate how good each channel is using comprehensive features\n",
        "            ('effectiveness_calculator', ChannelEffectivenessCalculator()),\n",
        "\n",
        "            # Step 2: Rank channels from best to worst\n",
        "            ('channel_ranker', ChannelRanker()),\n",
        "\n",
        "            # Step 3: Convert to model-friendly formats\n",
        "            ('label_encoder', PreferenceLabelEncoder())\n",
        "        ])\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Create labels from the engineered features\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"BUILDING CHANNEL PREFERENCE LABELS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Store original Customer_id and Agent IDs\n",
        "            original_columns = {}\n",
        "            important_columns = ['Customer_id', 'Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "\n",
        "            for col in important_columns:\n",
        "                if col in X.columns:\n",
        "                    original_columns[col] = X[col].copy()\n",
        "\n",
        "            # Apply the pipeline\n",
        "            X_with_labels = self.pipeline.fit_transform(X)\n",
        "\n",
        "            # Ensure important columns are preserved\n",
        "            for col, values in original_columns.items():\n",
        "                if col not in X_with_labels.columns:\n",
        "                    X_with_labels[col] = values\n",
        "\n",
        "            # Extract the important label columns\n",
        "            label_columns = ['Customer_id', 'Channel_Preference_Order',\n",
        "                           'Preference_Label', 'Top_Channel', 'Top_Channel_Label']\n",
        "\n",
        "            # Add binary preference columns\n",
        "            binary_cols = [col for col in X_with_labels.columns if 'Prefers_' in col]\n",
        "            label_columns.extend(binary_cols)\n",
        "\n",
        "            # Add agent columns if they exist\n",
        "            agent_cols = [col for col in ['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']\n",
        "                         if col in X_with_labels.columns]\n",
        "            label_columns.extend(agent_cols)\n",
        "\n",
        "            self.labels = X_with_labels[label_columns]\n",
        "\n",
        "            print(\"âœ… Label construction completed!\")\n",
        "            print(f\"ðŸ“Š Number of customers: {len(self.labels)}\")\n",
        "            print(f\"ðŸŽ¯ Number of unique preference patterns: {len(self.labels['Channel_Preference_Order'].unique())}\")\n",
        "\n",
        "            # Show what we created\n",
        "            self._analyze_labels()\n",
        "\n",
        "            return X_with_labels, self.labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in label construction: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return X, None\n",
        "\n",
        "    def _analyze_labels(self):\n",
        "        \"\"\"Analyze the created labels\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"LABEL ANALYSIS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Top channels overall\n",
        "        top_channels = self.labels['Top_Channel'].value_counts()\n",
        "        print(\"\\nðŸ† Most popular top channels:\")\n",
        "        for channel, count in top_channels.items():\n",
        "            percentage = (count / len(self.labels)) * 100\n",
        "            print(f\"   {channel}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "        # Preference order distribution\n",
        "        print(f\"\\nðŸŽ¯ Unique preference orders: {len(self.labels['Channel_Preference_Order'].unique())}\")\n",
        "\n",
        "        # Show most common preference patterns\n",
        "        common_orders = self.labels['Channel_Preference_Order'].value_counts().head(5)\n",
        "        print(\"\\nðŸ” Top 5 preference patterns:\")\n",
        "        for order, count in common_orders.items():\n",
        "            print(f\"   {order}: {count} customers\")\n",
        "\n",
        "        # Binary preference distribution\n",
        "        print(\"\\nðŸ“± Channel presence in Top 3:\")\n",
        "        for channel in ['Call', 'SMS', 'WhatsApp', 'Email']:\n",
        "            col = f'Prefers_{channel}_Top3'\n",
        "            if col in self.labels.columns:\n",
        "                count = self.labels[col].sum()\n",
        "                percentage = (count / len(self.labels)) * 100\n",
        "                print(f\"   {channel}: {count} customers ({percentage:.1f}%)\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Get the constructed labels\"\"\"\n",
        "        return self.labels\n",
        "\n",
        "    def save_labels(self, filepath='channel_preference_labels.csv'):\n",
        "        \"\"\"Save labels to CSV file\"\"\"\n",
        "        if self.labels is not None:\n",
        "            self.labels.to_csv(filepath, index=False)\n",
        "            print(f\"ðŸ’¾ Labels saved to {filepath}\")\n",
        "        else:\n",
        "            print(\"âŒ No labels to save\")\n",
        "\n",
        "    def get_training_data(self, features_df):\n",
        "        \"\"\"Prepare features and targets for model training\"\"\"\n",
        "        if self.labels is None:\n",
        "            print(\"No labels available. Run fit_transform first.\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Merge features with labels\n",
        "        training_data = features_df.merge(\n",
        "            self.labels[['Customer_id', 'Preference_Label', 'Top_Channel_Label', 'Top_Channel']],\n",
        "            on='Customer_id',\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        # Separate features and targets\n",
        "        feature_columns = [col for col in training_data.columns if col not in\n",
        "                         ['Customer_id', 'Preference_Label', 'Top_Channel_Label', 'Top_Channel',\n",
        "                          'Channel_Preference_Order', 'Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs']]\n",
        "\n",
        "        X = training_data[feature_columns]\n",
        "        y_preference = training_data['Preference_Label']  # Multi-class (full order)\n",
        "        y_top_channel = training_data['Top_Channel_Label']  # Multi-class (top channel only)\n",
        "\n",
        "        return X, y_preference, y_top_channel\n",
        "\n",
        "# ðŸš€ **MAIN EXECUTION**\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your engineered features\n",
        "    print(\"ðŸ” Loading engineered features...\")\n",
        "\n",
        "    try:\n",
        "        # Try to load the comprehensive engineered features\n",
        "        df = pd.read_csv('comprehensive_engineered_features.csv')\n",
        "        print(\"âœ… Loaded comprehensive_engineered_features.csv\")\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            # Fallback to processed features\n",
        "            df = pd.read_csv('processed_updated_data.csv')\n",
        "            print(\"âœ… Loaded processed_updated_data.csv\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"âŒ No feature file found. Please ensure you have run the feature engineering pipeline first.\")\n",
        "            exit()\n",
        "\n",
        "    print(f\"ðŸ“Š Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "    print(f\"ðŸ‘¤ Customer count: {df['Customer_id'].nunique() if 'Customer_id' in df.columns else 'N/A'}\")\n",
        "\n",
        "    # Ensure Customer_id exists\n",
        "    if 'Customer_id' not in df.columns:\n",
        "        print(\"âš ï¸ Creating temporary Customer_id...\")\n",
        "        df['Customer_id'] = [f'CUST_{i+1:06d}' for i in range(len(df))]\n",
        "\n",
        "    # Use the full comprehensive pipeline\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"USING COMPREHENSIVE LABELING PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    label_pipeline = LabelConstructionPipeline()\n",
        "    df_with_labels, labels = label_pipeline.fit_transform(df)\n",
        "\n",
        "    if labels is not None:\n",
        "        # Save results\n",
        "        labels.to_csv('channel_preference_labels.csv', index=False)\n",
        "        df_with_labels.to_csv('features_with_channel_labels.csv', index=False)\n",
        "\n",
        "        print(\"\\nðŸ’¾ Saved results:\")\n",
        "        print(\"   - channel_preference_labels.csv (just the labels)\")\n",
        "        print(\"   - features_with_channel_labels.csv (features + labels)\")\n",
        "\n",
        "        # Show sample of what we created\n",
        "        print(\"\\nðŸ“‹ Sample of created labels:\")\n",
        "        sample_cols = ['Customer_id', 'Top_Channel', 'Channel_Preference_Order', 'Preference_Label']\n",
        "        available_cols = [col for col in sample_cols if col in labels.columns]\n",
        "        print(labels[available_cols].head(10))\n",
        "\n",
        "        # Prepare training data (silently, without printing)\n",
        "        X, y_preference, y_top_channel = label_pipeline.get_training_data(df)\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Label creation failed!\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Channel preference labeling completed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIjbG3m4R9An",
        "outputId": "39ce7c90-2166-4698-f4f7-bf181c22baba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Loading engineered features...\n",
            "âœ… Loaded comprehensive_engineered_features.csv\n",
            "ðŸ“Š Loaded data: 100000 rows, 42 columns\n",
            "ðŸ‘¤ Customer count: 100000\n",
            "\n",
            "============================================================\n",
            "USING COMPREHENSIVE LABELING PIPELINE\n",
            "============================================================\n",
            "============================================================\n",
            "BUILDING CHANNEL PREFERENCE LABELS\n",
            "============================================================\n",
            "ðŸ“Š Calculating channel effectiveness scores...\n",
            "   âœ… Using Call_Efficiency for Call\n",
            "   âœ… Using SMS_Efficiency for SMS\n",
            "   âœ… Using WhatsApp_Efficiency for WhatsApp\n",
            "ðŸŽ¯ Ranking channels from best to worst...\n",
            "ðŸ·ï¸ Converting preferences to model-friendly formats...\n",
            "âœ… Label construction completed!\n",
            "ðŸ“Š Number of customers: 100000\n",
            "ðŸŽ¯ Number of unique preference patterns: 352\n",
            "\n",
            "==================================================\n",
            "LABEL ANALYSIS\n",
            "==================================================\n",
            "\n",
            "ðŸ† Most popular top channels:\n",
            "   Call: 43519 customers (43.5%)\n",
            "   SMS: 21552 customers (21.6%)\n",
            "   WhatsApp: 17458 customers (17.5%)\n",
            "   Field_Agent: 10991 customers (11.0%)\n",
            "   Email: 6257 customers (6.3%)\n",
            "   IVR: 223 customers (0.2%)\n",
            "\n",
            "ðŸŽ¯ Unique preference orders: 352\n",
            "\n",
            "ðŸ” Top 5 preference patterns:\n",
            "   Call,SMS,Field_Agent,IVR,Email,WhatsApp: 12832 customers\n",
            "   Call,Field_Agent,IVR,Email,WhatsApp,SMS: 8891 customers\n",
            "   SMS,Field_Agent,IVR,Email,Call,WhatsApp: 5433 customers\n",
            "   SMS,Email,Call,Field_Agent,IVR,WhatsApp: 5083 customers\n",
            "   Call,Field_Agent,IVR,Email,SMS,WhatsApp: 4876 customers\n",
            "\n",
            "ðŸ“± Channel presence in Top 3:\n",
            "   Call: 71067 customers (71.1%)\n",
            "   SMS: 48190 customers (48.2%)\n",
            "   WhatsApp: 24440 customers (24.4%)\n",
            "   Email: 39277 customers (39.3%)\n",
            "\n",
            "ðŸ’¾ Saved results:\n",
            "   - channel_preference_labels.csv (just the labels)\n",
            "   - features_with_channel_labels.csv (features + labels)\n",
            "\n",
            "ðŸ“‹ Sample of created labels:\n",
            "    Customer_id  Top_Channel                 Channel_Preference_Order  \\\n",
            "0  SCB843421788          SMS  SMS,Email,Call,Field_Agent,IVR,WhatsApp   \n",
            "1  SCB998027725         Call  Call,Field_Agent,IVR,Email,WhatsApp,SMS   \n",
            "2  SCB871158951         Call  Call,SMS,Field_Agent,IVR,Email,WhatsApp   \n",
            "3  SCB938686930  Field_Agent  Field_Agent,IVR,WhatsApp,Email,SMS,Call   \n",
            "4  SCB843983697          SMS  SMS,Email,Call,Field_Agent,IVR,WhatsApp   \n",
            "5  SCB946928259          SMS  SMS,Field_Agent,IVR,Email,Call,WhatsApp   \n",
            "6  SCB900671743         Call  Call,SMS,Email,Field_Agent,IVR,WhatsApp   \n",
            "7  SCB882059819         Call  Call,SMS,Field_Agent,IVR,Email,WhatsApp   \n",
            "8  SCB892183816          SMS  SMS,Field_Agent,IVR,Email,Call,WhatsApp   \n",
            "9  SCB831408653     WhatsApp  WhatsApp,Call,SMS,Field_Agent,IVR,Email   \n",
            "\n",
            "   Preference_Label  \n",
            "0               219  \n",
            "1                24  \n",
            "2                47  \n",
            "3               173  \n",
            "4               219  \n",
            "5               245  \n",
            "6                40  \n",
            "7                47  \n",
            "8               245  \n",
            "9               290  \n",
            "\n",
            "ðŸŽ‰ Channel preference labeling completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ChannelRankingDataPreparator:\n",
        "    \"\"\"Prepare data for XGBRanker training\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        self.label_encoders = {}\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def prepare_ranking_data(self, df):\n",
        "        \"\"\"Convert preference data to ranking format\"\"\"\n",
        "        print(\"Preparing ranking format...\")\n",
        "\n",
        "        # Get feature columns (exclude label columns and Customer_id)\n",
        "        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']\n",
        "        exclude_cols.extend([col for col in df.columns if 'Prefers_' in col])\n",
        "        exclude_cols.extend(['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs'])\n",
        "\n",
        "        self.feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Encode categorical columns\n",
        "        X = df[self.feature_cols].copy()\n",
        "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        if categorical_cols:\n",
        "            print(f\"Encoding {len(categorical_cols)} categorical columns...\")\n",
        "            for col in categorical_cols:\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        # Create ranking dataset\n",
        "        ranking_data = []\n",
        "        group_sizes = []\n",
        "\n",
        "        print(f\"Processing {len(df)} customers...\")\n",
        "\n",
        "        for idx, (_, row) in enumerate(df.iterrows()):\n",
        "            if (idx + 1) % 10000 == 0:\n",
        "                print(f\"Processed {idx + 1:,} customers\")\n",
        "\n",
        "            customer_id = row['Customer_id']\n",
        "            preference_order = row['Channel_Preference_Order'].split(',')\n",
        "\n",
        "            # Get customer features\n",
        "            customer_features = X.iloc[idx].values\n",
        "\n",
        "            # Create one sample per channel\n",
        "            for rank, channel in enumerate(preference_order):\n",
        "                if channel in self.channels:\n",
        "                    # Channel one-hot encoding\n",
        "                    channel_features = np.zeros(len(self.channels))\n",
        "                    channel_idx = self.channels.index(channel)\n",
        "                    channel_features[channel_idx] = 1\n",
        "\n",
        "                    # Combine customer and channel features\n",
        "                    combined_features = np.concatenate([customer_features, channel_features])\n",
        "\n",
        "                    # Relevance score as integer (required by XGBoost)\n",
        "                    # Best channel gets highest score, worst gets lowest\n",
        "                    relevance = len(self.channels) - rank\n",
        "\n",
        "                    ranking_data.append({\n",
        "                        'customer_id': customer_id,\n",
        "                        'channel': channel,\n",
        "                        'features': combined_features,\n",
        "                        'relevance': relevance,\n",
        "                        'group_id': idx\n",
        "                    })\n",
        "\n",
        "            group_sizes.append(len(self.channels))\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_ranking = np.array([item['features'] for item in ranking_data])\n",
        "        y_ranking = np.array([item['relevance'] for item in ranking_data])\n",
        "        groups = np.array(group_sizes)\n",
        "\n",
        "        print(f\"\\nRanking dataset created:\")\n",
        "        print(f\"Total samples: {len(X_ranking):,}\")\n",
        "        print(f\"Total customers (groups): {len(groups):,}\")\n",
        "        print(f\"Features per sample: {X_ranking.shape[1]}\")\n",
        "        print(f\"Group sizes (samples per customer): {groups[0]} (all should be {len(self.channels)})\")\n",
        "        print(f\"Relevance score range: {y_ranking.min()} to {y_ranking.max()}\")\n",
        "\n",
        "        return X_ranking, y_ranking, groups\n",
        "\n",
        "def train_xgb_ranker(X, y, groups, test_size=0.2, random_state=42):\n",
        "    \"\"\"Train XGBRanker model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING XGBRANKER MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate split points for groups\n",
        "    n_train_groups = int(len(groups) * (1 - test_size))\n",
        "    train_samples = sum(groups[:n_train_groups])\n",
        "\n",
        "    # Split data\n",
        "    X_train = X[:train_samples]\n",
        "    y_train = y[:train_samples]\n",
        "    groups_train = groups[:n_train_groups]\n",
        "\n",
        "    X_test = X[train_samples:]\n",
        "    y_test = y[train_samples:]\n",
        "    groups_test = groups[n_train_groups:]\n",
        "\n",
        "    print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "    print(f\"Label vector shape: {y_train.shape}\")\n",
        "    print(f\"Number of groups: {len(groups_train):,}\")\n",
        "    print(f\"\\nTrain set: {len(X_train):,} samples from {len(groups_train):,} customers\")\n",
        "    print(f\"Test set: {len(X_test):,} samples from {len(groups_test):,} customers\")\n",
        "\n",
        "    # Initialize XGBRanker\n",
        "    print(\"\\nInitializing XGBRanker...\")\n",
        "    model = xgb.XGBRanker(\n",
        "        objective='rank:ndcg',\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        n_estimators=100,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=random_state,\n",
        "        eval_metric='ndcg@6'\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training XGBRanker...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        group=groups_train,\n",
        "        eval_set=[(X_test, y_test)],\n",
        "        eval_group=[groups_test],\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel training completed successfully!\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate NDCG\n",
        "    print(\"Calculating NDCG score...\")\n",
        "    # Reshape predictions for NDCG calculation\n",
        "    y_test_reshaped = []\n",
        "    y_pred_reshaped = []\n",
        "\n",
        "    start_idx = 0\n",
        "    for group_size in groups_test:\n",
        "        end_idx = start_idx + group_size\n",
        "        y_test_reshaped.append(y_test[start_idx:end_idx])\n",
        "        y_pred_reshaped.append(y_pred[start_idx:end_idx])\n",
        "        start_idx = end_idx\n",
        "\n",
        "    ndcg = ndcg_score(y_test_reshaped, y_pred_reshaped)\n",
        "    print(f\"NDCG Score: {ndcg:.4f}\")\n",
        "\n",
        "    return model, ndcg\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"PREPARING CHANNEL RANKING DATA FOR XGBRANKER\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading features_with_channel_labels.csv...\")\n",
        "    try:\n",
        "        df = pd.read_csv('features_with_channel_labels.csv')\n",
        "        print(f\"Loaded data shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: features_with_channel_labels.csv not found!\")\n",
        "        print(\"Please run the labeling pipeline first.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Prepare data\n",
        "    preparator = ChannelRankingDataPreparator()\n",
        "    X, y, groups = preparator.prepare_ranking_data(df)\n",
        "\n",
        "    # Train model\n",
        "    model, ndcg_score = train_xgb_ranker(X, y, groups)\n",
        "\n",
        "    # Save model only (no preparator)\n",
        "    model_filename = 'xgb_channel_ranker.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"\\nModel saved as '{model_filename}'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Feature columns used: {X.shape[1]}\")\n",
        "    print(f\"Model saved as: {model_filename}\")\n",
        "    print(f\"Total customers processed: {len(df):,}\")\n",
        "    print(f\"Final NDCG Score: {ndcg_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfwK7pHVUwAK",
        "outputId": "8b1d71fe-7436-4a47-8048-e2b9c15dbbcb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PREPARING CHANNEL RANKING DATA FOR XGBRANKER\n",
            "============================================================\n",
            "Loading features_with_channel_labels.csv...\n",
            "Loaded data shape: (100000, 56)\n",
            "Preparing ranking format...\n",
            "Encoding 1 categorical columns...\n",
            "Processing 100000 customers...\n",
            "Processed 10,000 customers\n",
            "Processed 20,000 customers\n",
            "Processed 30,000 customers\n",
            "Processed 40,000 customers\n",
            "Processed 50,000 customers\n",
            "Processed 60,000 customers\n",
            "Processed 70,000 customers\n",
            "Processed 80,000 customers\n",
            "Processed 90,000 customers\n",
            "Processed 100,000 customers\n",
            "\n",
            "Ranking dataset created:\n",
            "Total samples: 600,000\n",
            "Total customers (groups): 100,000\n",
            "Features per sample: 52\n",
            "Group sizes (samples per customer): 6 (all should be 6)\n",
            "Relevance score range: 1 to 6\n",
            "\n",
            "============================================================\n",
            "TRAINING XGBRANKER MODEL\n",
            "============================================================\n",
            "Feature matrix shape: (480000, 52)\n",
            "Label vector shape: (480000,)\n",
            "Number of groups: 80,000\n",
            "\n",
            "Train set: 480,000 samples from 80,000 customers\n",
            "Test set: 120,000 samples from 20,000 customers\n",
            "\n",
            "Initializing XGBRanker...\n",
            "Training XGBRanker...\n",
            "[0]\tvalidation_0-ndcg@6:0.91221\n",
            "[1]\tvalidation_0-ndcg@6:0.98047\n",
            "[2]\tvalidation_0-ndcg@6:0.98493\n",
            "[3]\tvalidation_0-ndcg@6:0.98955\n",
            "[4]\tvalidation_0-ndcg@6:0.99107\n",
            "[5]\tvalidation_0-ndcg@6:0.99194\n",
            "[6]\tvalidation_0-ndcg@6:0.99241\n",
            "[7]\tvalidation_0-ndcg@6:0.99285\n",
            "[8]\tvalidation_0-ndcg@6:0.99274\n",
            "[9]\tvalidation_0-ndcg@6:0.99424\n",
            "[10]\tvalidation_0-ndcg@6:0.99397\n",
            "[11]\tvalidation_0-ndcg@6:0.99458\n",
            "[12]\tvalidation_0-ndcg@6:0.99492\n",
            "[13]\tvalidation_0-ndcg@6:0.99546\n",
            "[14]\tvalidation_0-ndcg@6:0.99541\n",
            "[15]\tvalidation_0-ndcg@6:0.99568\n",
            "[16]\tvalidation_0-ndcg@6:0.99607\n",
            "[17]\tvalidation_0-ndcg@6:0.99600\n",
            "[18]\tvalidation_0-ndcg@6:0.99595\n",
            "[19]\tvalidation_0-ndcg@6:0.99604\n",
            "[20]\tvalidation_0-ndcg@6:0.99626\n",
            "[21]\tvalidation_0-ndcg@6:0.99649\n",
            "[22]\tvalidation_0-ndcg@6:0.99674\n",
            "[23]\tvalidation_0-ndcg@6:0.99700\n",
            "[24]\tvalidation_0-ndcg@6:0.99708\n",
            "[25]\tvalidation_0-ndcg@6:0.99707\n",
            "[26]\tvalidation_0-ndcg@6:0.99703\n",
            "[27]\tvalidation_0-ndcg@6:0.99708\n",
            "[28]\tvalidation_0-ndcg@6:0.99716\n",
            "[29]\tvalidation_0-ndcg@6:0.99718\n",
            "[30]\tvalidation_0-ndcg@6:0.99729\n",
            "[31]\tvalidation_0-ndcg@6:0.99733\n",
            "[32]\tvalidation_0-ndcg@6:0.99735\n",
            "[33]\tvalidation_0-ndcg@6:0.99739\n",
            "[34]\tvalidation_0-ndcg@6:0.99741\n",
            "[35]\tvalidation_0-ndcg@6:0.99743\n",
            "[36]\tvalidation_0-ndcg@6:0.99749\n",
            "[37]\tvalidation_0-ndcg@6:0.99754\n",
            "[38]\tvalidation_0-ndcg@6:0.99760\n",
            "[39]\tvalidation_0-ndcg@6:0.99764\n",
            "[40]\tvalidation_0-ndcg@6:0.99766\n",
            "[41]\tvalidation_0-ndcg@6:0.99766\n",
            "[42]\tvalidation_0-ndcg@6:0.99768\n",
            "[43]\tvalidation_0-ndcg@6:0.99773\n",
            "[44]\tvalidation_0-ndcg@6:0.99774\n",
            "[45]\tvalidation_0-ndcg@6:0.99778\n",
            "[46]\tvalidation_0-ndcg@6:0.99779\n",
            "[47]\tvalidation_0-ndcg@6:0.99782\n",
            "[48]\tvalidation_0-ndcg@6:0.99782\n",
            "[49]\tvalidation_0-ndcg@6:0.99786\n",
            "[50]\tvalidation_0-ndcg@6:0.99790\n",
            "[51]\tvalidation_0-ndcg@6:0.99791\n",
            "[52]\tvalidation_0-ndcg@6:0.99795\n",
            "[53]\tvalidation_0-ndcg@6:0.99795\n",
            "[54]\tvalidation_0-ndcg@6:0.99795\n",
            "[55]\tvalidation_0-ndcg@6:0.99798\n",
            "[56]\tvalidation_0-ndcg@6:0.99800\n",
            "[57]\tvalidation_0-ndcg@6:0.99801\n",
            "[58]\tvalidation_0-ndcg@6:0.99804\n",
            "[59]\tvalidation_0-ndcg@6:0.99807\n",
            "[60]\tvalidation_0-ndcg@6:0.99809\n",
            "[61]\tvalidation_0-ndcg@6:0.99815\n",
            "[62]\tvalidation_0-ndcg@6:0.99816\n",
            "[63]\tvalidation_0-ndcg@6:0.99821\n",
            "[64]\tvalidation_0-ndcg@6:0.99827\n",
            "[65]\tvalidation_0-ndcg@6:0.99838\n",
            "[66]\tvalidation_0-ndcg@6:0.99842\n",
            "[67]\tvalidation_0-ndcg@6:0.99844\n",
            "[68]\tvalidation_0-ndcg@6:0.99846\n",
            "[69]\tvalidation_0-ndcg@6:0.99849\n",
            "[70]\tvalidation_0-ndcg@6:0.99851\n",
            "[71]\tvalidation_0-ndcg@6:0.99852\n",
            "[72]\tvalidation_0-ndcg@6:0.99855\n",
            "[73]\tvalidation_0-ndcg@6:0.99858\n",
            "[74]\tvalidation_0-ndcg@6:0.99865\n",
            "[75]\tvalidation_0-ndcg@6:0.99878\n",
            "[76]\tvalidation_0-ndcg@6:0.99886\n",
            "[77]\tvalidation_0-ndcg@6:0.99891\n",
            "[78]\tvalidation_0-ndcg@6:0.99895\n",
            "[79]\tvalidation_0-ndcg@6:0.99900\n",
            "[80]\tvalidation_0-ndcg@6:0.99901\n",
            "[81]\tvalidation_0-ndcg@6:0.99906\n",
            "[82]\tvalidation_0-ndcg@6:0.99908\n",
            "[83]\tvalidation_0-ndcg@6:0.99908\n",
            "[84]\tvalidation_0-ndcg@6:0.99912\n",
            "[85]\tvalidation_0-ndcg@6:0.99914\n",
            "[86]\tvalidation_0-ndcg@6:0.99919\n",
            "[87]\tvalidation_0-ndcg@6:0.99921\n",
            "[88]\tvalidation_0-ndcg@6:0.99923\n",
            "[89]\tvalidation_0-ndcg@6:0.99926\n",
            "[90]\tvalidation_0-ndcg@6:0.99928\n",
            "[91]\tvalidation_0-ndcg@6:0.99930\n",
            "[92]\tvalidation_0-ndcg@6:0.99933\n",
            "[93]\tvalidation_0-ndcg@6:0.99936\n",
            "[94]\tvalidation_0-ndcg@6:0.99938\n",
            "[95]\tvalidation_0-ndcg@6:0.99938\n",
            "[96]\tvalidation_0-ndcg@6:0.99941\n",
            "[97]\tvalidation_0-ndcg@6:0.99942\n",
            "[98]\tvalidation_0-ndcg@6:0.99941\n",
            "[99]\tvalidation_0-ndcg@6:0.99943\n",
            "\n",
            "Model training completed successfully!\n",
            "\n",
            "Making predictions...\n",
            "Calculating NDCG score...\n",
            "NDCG Score: 0.9991\n",
            "\n",
            "Model saved as 'xgb_channel_ranker.pkl'\n",
            "\n",
            "============================================================\n",
            "MODEL TRAINING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Feature columns used: 52\n",
            "Model saved as: xgb_channel_ranker.pkl\n",
            "Total customers processed: 100,000\n",
            "Final NDCG Score: 0.9991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ChannelRankingDataPreparator:\n",
        "    \"\"\"Prepare data for LightGBM Ranker training\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        self.label_encoders = {}\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def prepare_ranking_data(self, df):\n",
        "        \"\"\"Convert preference data to ranking format\"\"\"\n",
        "        print(\"Preparing ranking format...\")\n",
        "\n",
        "        # Get feature columns (exclude label columns and Customer_id)\n",
        "        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']\n",
        "        exclude_cols.extend([col for col in df.columns if 'Prefers_' in col])\n",
        "        exclude_cols.extend(['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs'])\n",
        "\n",
        "        self.feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Encode categorical columns\n",
        "        X = df[self.feature_cols].copy()\n",
        "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        if categorical_cols:\n",
        "            print(f\"Encoding {len(categorical_cols)} categorical columns...\")\n",
        "            for col in categorical_cols:\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        # Create ranking dataset\n",
        "        ranking_data = []\n",
        "        group_sizes = []\n",
        "\n",
        "        print(f\"Processing {len(df)} customers...\")\n",
        "\n",
        "        for idx, (_, row) in enumerate(df.iterrows()):\n",
        "            if (idx + 1) % 10000 == 0:\n",
        "                print(f\"Processed {idx + 1:,} customers\")\n",
        "\n",
        "            customer_id = row['Customer_id']\n",
        "            preference_order = row['Channel_Preference_Order'].split(',')\n",
        "\n",
        "            # Get customer features\n",
        "            customer_features = X.iloc[idx].values\n",
        "\n",
        "            # Create one sample per channel\n",
        "            for rank, channel in enumerate(preference_order):\n",
        "                if channel in self.channels:\n",
        "                    # Channel one-hot encoding\n",
        "                    channel_features = np.zeros(len(self.channels))\n",
        "                    channel_idx = self.channels.index(channel)\n",
        "                    channel_features[channel_idx] = 1\n",
        "\n",
        "                    # Combine customer and channel features\n",
        "                    combined_features = np.concatenate([customer_features, channel_features])\n",
        "\n",
        "                    # Relevance score as integer (required by LightGBM)\n",
        "                    # Best channel gets highest score, worst gets lowest\n",
        "                    relevance = len(self.channels) - rank\n",
        "\n",
        "                    ranking_data.append({\n",
        "                        'customer_id': customer_id,\n",
        "                        'channel': channel,\n",
        "                        'features': combined_features,\n",
        "                        'relevance': relevance,\n",
        "                        'group_id': idx\n",
        "                    })\n",
        "\n",
        "            group_sizes.append(len(self.channels))\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_ranking = np.array([item['features'] for item in ranking_data])\n",
        "        y_ranking = np.array([item['relevance'] for item in ranking_data])\n",
        "        groups = np.array(group_sizes)\n",
        "\n",
        "        print(f\"\\nRanking dataset created:\")\n",
        "        print(f\"Total samples: {len(X_ranking):,}\")\n",
        "        print(f\"Total customers (groups): {len(groups):,}\")\n",
        "        print(f\"Features per sample: {X_ranking.shape[1]}\")\n",
        "        print(f\"Group sizes (samples per customer): {groups[0]} (all should be {len(self.channels)})\")\n",
        "        print(f\"Relevance score range: {y_ranking.min()} to {y_ranking.max()}\")\n",
        "\n",
        "        return X_ranking, y_ranking, groups\n",
        "\n",
        "def train_lightgbm_ranker(X, y, groups, test_size=0.2, random_state=42):\n",
        "    \"\"\"Train LightGBM Ranker model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING LIGHTGBM RANKER MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate split points for groups\n",
        "    n_train_groups = int(len(groups) * (1 - test_size))\n",
        "    train_samples = sum(groups[:n_train_groups])\n",
        "\n",
        "    # Split data\n",
        "    X_train = X[:train_samples]\n",
        "    y_train = y[:train_samples]\n",
        "    groups_train = groups[:n_train_groups]\n",
        "\n",
        "    X_test = X[train_samples:]\n",
        "    y_test = y[train_samples:]\n",
        "    groups_test = groups[n_train_groups:]\n",
        "\n",
        "    print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "    print(f\"Label vector shape: {y_train.shape}\")\n",
        "    print(f\"Number of groups: {len(groups_train):,}\")\n",
        "    print(f\"\\nTrain set: {len(X_train):,} samples from {len(groups_train):,} customers\")\n",
        "    print(f\"Test set: {len(X_test):,} samples from {len(groups_test):,} customers\")\n",
        "\n",
        "    # Create LightGBM datasets\n",
        "    print(\"\\nCreating LightGBM datasets...\")\n",
        "    train_data = lgb.Dataset(\n",
        "        X_train,\n",
        "        label=y_train,\n",
        "        group=groups_train,\n",
        "        free_raw_data=False\n",
        "    )\n",
        "\n",
        "    test_data = lgb.Dataset(\n",
        "        X_test,\n",
        "        label=y_test,\n",
        "        group=groups_test,\n",
        "        free_raw_data=False,\n",
        "        reference=train_data\n",
        "    )\n",
        "\n",
        "    # LightGBM parameters for ranking\n",
        "    params = {\n",
        "        'objective': 'lambdarank',\n",
        "        'metric': 'ndcg',\n",
        "        'ndcg_eval_at': [6],\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'max_depth': 6,\n",
        "        'min_data_in_leaf': 20,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'lambda_l1': 0.1,\n",
        "        'lambda_l2': 0.1,\n",
        "        'random_state': random_state,\n",
        "        'verbosity': -1\n",
        "    }\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training LightGBM Ranker...\")\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[train_data, test_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[\n",
        "            lgb.log_evaluation(50),  # Print every 50 rounds\n",
        "            lgb.early_stopping(20)   # Early stopping if no improvement for 20 rounds\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel training completed successfully!\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate NDCG\n",
        "    print(\"Calculating NDCG score...\")\n",
        "    # Reshape predictions for NDCG calculation\n",
        "    y_test_reshaped = []\n",
        "    y_pred_reshaped = []\n",
        "\n",
        "    start_idx = 0\n",
        "    for group_size in groups_test:\n",
        "        end_idx = start_idx + group_size\n",
        "        y_test_reshaped.append(y_test[start_idx:end_idx])\n",
        "        y_pred_reshaped.append(y_pred[start_idx:end_idx])\n",
        "        start_idx = end_idx\n",
        "\n",
        "    ndcg = ndcg_score(y_test_reshaped, y_pred_reshaped)\n",
        "    print(f\"NDCG Score: {ndcg:.4f}\")\n",
        "\n",
        "    return model, ndcg\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"PREPARING CHANNEL RANKING DATA FOR LIGHTGBM RANKER\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading features_with_channel_labels.csv...\")\n",
        "    try:\n",
        "        df = pd.read_csv('features_with_channel_labels.csv')\n",
        "        print(f\"Loaded data shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: features_with_channel_labels.csv not found!\")\n",
        "        print(\"Please run the labeling pipeline first.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Prepare data\n",
        "    preparator = ChannelRankingDataPreparator()\n",
        "    X, y, groups = preparator.prepare_ranking_data(df)\n",
        "\n",
        "    # Train model\n",
        "    model, ndcg_score = train_lightgbm_ranker(X, y, groups)\n",
        "\n",
        "    # Save model only (no preparator)\n",
        "    model_filename = 'lightgbm_channel_ranker.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"\\nModel saved as '{model_filename}'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Feature columns used: {X.shape[1]}\")\n",
        "    print(f\"Model saved as: {model_filename}\")\n",
        "    print(f\"Total customers processed: {len(df):,}\")\n",
        "    print(f\"Final NDCG Score: {ndcg_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KcxPPJSV3WV",
        "outputId": "775818d1-b96f-48c6-81ad-efa233491065"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PREPARING CHANNEL RANKING DATA FOR LIGHTGBM RANKER\n",
            "============================================================\n",
            "Loading features_with_channel_labels.csv...\n",
            "Loaded data shape: (100000, 56)\n",
            "Preparing ranking format...\n",
            "Encoding 1 categorical columns...\n",
            "Processing 100000 customers...\n",
            "Processed 10,000 customers\n",
            "Processed 20,000 customers\n",
            "Processed 30,000 customers\n",
            "Processed 40,000 customers\n",
            "Processed 50,000 customers\n",
            "Processed 60,000 customers\n",
            "Processed 70,000 customers\n",
            "Processed 80,000 customers\n",
            "Processed 90,000 customers\n",
            "Processed 100,000 customers\n",
            "\n",
            "Ranking dataset created:\n",
            "Total samples: 600,000\n",
            "Total customers (groups): 100,000\n",
            "Features per sample: 52\n",
            "Group sizes (samples per customer): 6 (all should be 6)\n",
            "Relevance score range: 1 to 6\n",
            "\n",
            "============================================================\n",
            "TRAINING LIGHTGBM RANKER MODEL\n",
            "============================================================\n",
            "Feature matrix shape: (480000, 52)\n",
            "Label vector shape: (480000,)\n",
            "Number of groups: 80,000\n",
            "\n",
            "Train set: 480,000 samples from 80,000 customers\n",
            "Test set: 120,000 samples from 20,000 customers\n",
            "\n",
            "Creating LightGBM datasets...\n",
            "Training LightGBM Ranker...\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[50]\ttrain's ndcg@6: 0.999058\tvalid's ndcg@6: 0.999044\n",
            "[100]\ttrain's ndcg@6: 0.999856\tvalid's ndcg@6: 0.999813\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\ttrain's ndcg@6: 0.999856\tvalid's ndcg@6: 0.999813\n",
            "\n",
            "Model training completed successfully!\n",
            "\n",
            "Making predictions...\n",
            "Calculating NDCG score...\n",
            "NDCG Score: 0.9998\n",
            "\n",
            "Model saved as 'lightgbm_channel_ranker.pkl'\n",
            "\n",
            "============================================================\n",
            "MODEL TRAINING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Feature columns used: 52\n",
            "Model saved as: lightgbm_channel_ranker.pkl\n",
            "Total customers processed: 100,000\n",
            "Final NDCG Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class LightGBMPredictor:\n",
        "    \"\"\"LightGBM model predictor for channel preference ranking\"\"\"\n",
        "\n",
        "    def __init__(self, model_path='lightgbm_channel_ranker.pkl'):\n",
        "        \"\"\"\n",
        "        Initialize the LightGBM predictor\n",
        "        \"\"\"\n",
        "        print(\"ðŸ”§ Initializing LightGBM Predictor...\")\n",
        "        try:\n",
        "            # Load the model\n",
        "            self.model = joblib.load(model_path)\n",
        "            print(\"âœ… LightGBM model loaded successfully!\")\n",
        "\n",
        "            # Define channels (same as training)\n",
        "            self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "            print(f\"   Channels: {self.channels}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"âŒ Model file not found. Please ensure the model file exists.\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_customer_data(self, data_path, customer_id=None):\n",
        "        \"\"\"\n",
        "        Load customer data for prediction\n",
        "        \"\"\"\n",
        "        print(f\"ðŸ“ Loading customer data from {data_path}...\")\n",
        "        try:\n",
        "            self.df = pd.read_csv(data_path)\n",
        "            print(f\"âœ… Loaded {len(self.df)} customers\")\n",
        "            print(f\"âœ… Columns in data: {len(self.df.columns)}\")\n",
        "\n",
        "            if customer_id:\n",
        "                self.customer_data = self.df[self.df['Customer_id'] == customer_id]\n",
        "                if len(self.customer_data) == 0:\n",
        "                    available_ids = self.df['Customer_id'].head(5).tolist()\n",
        "                    raise ValueError(f\"Customer ID '{customer_id}' not found! Available: {available_ids}\")\n",
        "                print(f\"ðŸŽ¯ Predicting for customer: {customer_id}\")\n",
        "            else:\n",
        "                self.customer_data = self.df\n",
        "                print(f\"ðŸŽ¯ Predicting for all {len(self.customer_data)} customers\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading customer data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def load_processed_data(self, data_path):\n",
        "        \"\"\"\n",
        "        Load processed data for age and income band information\n",
        "        \"\"\"\n",
        "        print(f\"ðŸ“ Loading processed data from {data_path}...\")\n",
        "        try:\n",
        "            self.processed_df = pd.read_csv(data_path)\n",
        "            print(f\"âœ… Loaded {len(self.processed_df)} customers from processed data\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading processed data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def prepare_features(self, customer_data):\n",
        "        \"\"\"\n",
        "        Prepare features for prediction\n",
        "        \"\"\"\n",
        "        # Ensure we have a DataFrame\n",
        "        if isinstance(customer_data, pd.Series):\n",
        "            customer_data = customer_data.to_frame().T\n",
        "\n",
        "        # Get feature columns (exclude label and ID columns)\n",
        "        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']\n",
        "        exclude_cols.extend([col for col in customer_data.columns if 'Prefers_' in col])\n",
        "        exclude_cols.extend(['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs'])\n",
        "\n",
        "        feature_cols = [col for col in customer_data.columns if col not in exclude_cols]\n",
        "\n",
        "        print(f\"   ðŸ”§ Preparing {len(feature_cols)} features...\")\n",
        "\n",
        "        # Extract features\n",
        "        X_customer = customer_data[feature_cols].copy()\n",
        "\n",
        "        # Encode categorical features\n",
        "        categorical_cols = X_customer.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        if categorical_cols:\n",
        "            print(f\"   ðŸ”§ Encoding {len(categorical_cols)} categorical columns...\")\n",
        "            for col in categorical_cols:\n",
        "                try:\n",
        "                    # Use LabelEncoder for categorical columns\n",
        "                    le = LabelEncoder()\n",
        "                    X_customer[col] = le.fit_transform(X_customer[col].astype(str))\n",
        "                except Exception as e:\n",
        "                    print(f\"   âš ï¸  Encoding issue with {col}: {e}\")\n",
        "                    # Use default value for encoding issues\n",
        "                    X_customer[col] = 0\n",
        "\n",
        "        # Return flattened array for single customer\n",
        "        return X_customer.values.flatten(), feature_cols\n",
        "\n",
        "    def predict_single_customer(self, customer_data):\n",
        "        \"\"\"\n",
        "        Predict channel preferences for a single customer with 0-1 normalized scores\n",
        "        \"\"\"\n",
        "        print(\"   ðŸŽ¯ Making predictions...\")\n",
        "\n",
        "        # Ensure proper DataFrame format\n",
        "        if isinstance(customer_data, pd.Series):\n",
        "            customer_data = customer_data.to_frame().T\n",
        "\n",
        "        customer_id = customer_data['Customer_id'].iloc[0]\n",
        "\n",
        "        # Prepare customer features\n",
        "        customer_features, feature_cols = self.prepare_features(customer_data)\n",
        "\n",
        "        # Create samples for each channel\n",
        "        channel_predictions = []\n",
        "\n",
        "        for channel in self.channels:\n",
        "            # Channel one-hot encoding\n",
        "            channel_features = np.zeros(len(self.channels))\n",
        "            channel_idx = self.channels.index(channel)\n",
        "            channel_features[channel_idx] = 1\n",
        "\n",
        "            # Combine customer and channel features\n",
        "            combined_features = np.concatenate([customer_features, channel_features])\n",
        "\n",
        "            # Make prediction\n",
        "            try:\n",
        "                prediction_score = self.model.predict(combined_features.reshape(1, -1))[0]\n",
        "                channel_predictions.append({\n",
        "                    'channel': channel,\n",
        "                    'score': prediction_score,\n",
        "                    'customer_id': customer_id\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Prediction error for {channel}: {e}\")\n",
        "                channel_predictions.append({\n",
        "                    'channel': channel,\n",
        "                    'score': 0.0,\n",
        "                    'customer_id': customer_id\n",
        "                })\n",
        "\n",
        "        # Sort channels by prediction score (descending)\n",
        "        channel_predictions.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        # Normalize scores to 0-1 range using softmax\n",
        "        scores = np.array([pred['score'] for pred in channel_predictions])\n",
        "\n",
        "        # Apply softmax to get probabilities between 0-1\n",
        "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
        "        probabilities = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "        # Update scores with normalized values\n",
        "        for i, pred in enumerate(channel_predictions):\n",
        "            pred['score'] = probabilities[i]\n",
        "\n",
        "        print(f\"   âœ… Predictions completed for {customer_id}\")\n",
        "        return channel_predictions\n",
        "\n",
        "    def predict_multiple_customers(self, customer_ids=None):\n",
        "        \"\"\"\n",
        "        Predict channel preferences for multiple customers\n",
        "        \"\"\"\n",
        "        if customer_ids:\n",
        "            customers_to_predict = self.df[self.df['Customer_id'].isin(customer_ids)]\n",
        "            if len(customers_to_predict) == 0:\n",
        "                print(\"âŒ No matching customer IDs found!\")\n",
        "                return {}\n",
        "            print(f\"ðŸ” Predicting for {len(customers_to_predict)} specific customers...\")\n",
        "        else:\n",
        "            customers_to_predict = self.customer_data\n",
        "            print(f\"ðŸ” Predicting for all {len(customers_to_predict)} customers...\")\n",
        "\n",
        "        all_predictions = {}\n",
        "\n",
        "        for idx, (_, customer_row) in enumerate(customers_to_predict.iterrows()):\n",
        "            customer_df = pd.DataFrame([customer_row])\n",
        "            predictions = self.predict_single_customer(customer_df)\n",
        "            all_predictions[customer_row['Customer_id']] = predictions\n",
        "\n",
        "            if (idx + 1) % 100 == 0 and idx > 0:\n",
        "                print(f\"   ðŸ“Š Processed {idx + 1} customers...\")\n",
        "\n",
        "        return all_predictions\n",
        "\n",
        "    def format_predictions(self, predictions, top_n=6):\n",
        "        \"\"\"\n",
        "        Format predictions in a user-friendly way with 0-1 scores\n",
        "        \"\"\"\n",
        "        if not predictions:\n",
        "            return \"âŒ No predictions available.\"\n",
        "\n",
        "        if isinstance(predictions, list):\n",
        "            # Single customer prediction\n",
        "            result = f\"\\nðŸŽ¯ CHANNEL PREFERENCE PREDICTION\\n\"\n",
        "            result += \"=\" * 50 + \"\\n\"\n",
        "            result += f\"Customer: {predictions[0]['customer_id']}\\n\"\n",
        "            result += \"=\" * 50 + \"\\n\"\n",
        "\n",
        "            for i, pred in enumerate(predictions[:top_n], 1):\n",
        "                stars = \"â˜…\" * min(i, 5)\n",
        "                # Format score as percentage between 0-1 with 4 decimal places\n",
        "                result += f\"{i:2d}. {pred['channel']:12} Score: {pred['score']:7.4f} {stars}\\n\"\n",
        "\n",
        "            # Add confidence analysis based on normalized scores\n",
        "            confidence_gap = predictions[0]['score'] - predictions[1]['score']\n",
        "            result += f\"\\nðŸ’¡ Confidence: \"\n",
        "            if confidence_gap > 0.3:\n",
        "                result += \"HIGH (Clear preference)\\n\"\n",
        "            elif confidence_gap > 0.15:\n",
        "                result += \"MEDIUM (Strong preference)\\n\"\n",
        "            else:\n",
        "                result += \"LOW (Consider multiple channels)\\n\"\n",
        "\n",
        "            # Add recommendation\n",
        "            top_channel = predictions[0]['channel']\n",
        "            result += f\"ðŸš€ RECOMMENDATION: Start with {top_channel}\\n\"\n",
        "\n",
        "        else:\n",
        "            # Multiple customers prediction\n",
        "            result = f\"\\nðŸ“Š BATCH PREDICTION RESULTS ({len(predictions)} customers)\\n\"\n",
        "            result += \"=\" * 60 + \"\\n\"\n",
        "\n",
        "            for i, (customer_id, customer_predictions) in enumerate(list(predictions.items())[:10]):\n",
        "                result += f\"\\n{i+1:2d}. ðŸ‘¤ {customer_id}:\\n\"\n",
        "                result += f\"    ðŸ¥‡ {customer_predictions[0]['channel']} \"\n",
        "                result += f\"(Score: {customer_predictions[0]['score']:.4f})\\n\"\n",
        "                if len(customer_predictions) > 2:\n",
        "                    result += f\"    ðŸ“‹ Strategy: {customer_predictions[0]['channel']} â†’ \"\n",
        "                    result += f\"{customer_predictions[1]['channel']} â†’ \"\n",
        "                    result += f\"{customer_predictions[2]['channel']}\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_customer_insights(self, customer_id):\n",
        "        \"\"\"\n",
        "        Get additional insights about a customer from processed_data.csv\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'processed_df'):\n",
        "            return None\n",
        "\n",
        "        customer_row = self.processed_df[self.processed_df['Customer_id'] == customer_id]\n",
        "        if customer_row.empty:\n",
        "            return None\n",
        "\n",
        "        insights = {\n",
        "            'customer_id': customer_id,\n",
        "            'age': customer_row['Age'].iloc[0] if 'Age' in customer_row.columns else 'N/A',\n",
        "            'income_band': customer_row['Income_Band_SGD'].iloc[0] if 'Income_Band_SGD' in customer_row.columns else 'N/A'\n",
        "        }\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def save_predictions_to_csv(self, predictions, output_path='lightgbm_predictions.csv'):\n",
        "        \"\"\"\n",
        "        Save predictions to CSV file\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(predictions, list):\n",
        "                # Single customer\n",
        "                df_output = pd.DataFrame(predictions)\n",
        "            else:\n",
        "                # Multiple customers\n",
        "                all_predictions = []\n",
        "                for customer_id, customer_predictions in predictions.items():\n",
        "                    for i, pred in enumerate(customer_predictions, 1):\n",
        "                        pred['rank'] = i\n",
        "                        pred['customer_id'] = customer_id\n",
        "                        all_predictions.append(pred)\n",
        "\n",
        "                df_output = pd.DataFrame(all_predictions)\n",
        "\n",
        "            df_output.to_csv(output_path, index=False)\n",
        "            print(f\"ðŸ’¾ Predictions saved to {output_path}\")\n",
        "            return df_output\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error saving predictions: {e}\")\n",
        "            return None\n",
        "\n",
        "def predict_single_customer(customer_id):\n",
        "    \"\"\"Prediction function for a single customer\"\"\"\n",
        "    print(f\"\\nðŸ” PREDICTION FOR: {customer_id}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Initialize predictor\n",
        "        predictor = LightGBMPredictor('lightgbm_channel_ranker.pkl')\n",
        "\n",
        "        # Load customer data for prediction\n",
        "        predictor.load_customer_data('features_with_channel_labels.csv', customer_id)\n",
        "\n",
        "        # Load processed data for age and income band\n",
        "        predictor.load_processed_data('processed_data.csv')\n",
        "\n",
        "        # Predict\n",
        "        predictions = predictor.predict_single_customer(predictor.customer_data)\n",
        "\n",
        "        # Format results\n",
        "        result = predictor.format_predictions(predictions)\n",
        "\n",
        "        # Add customer insights (age and income band only)\n",
        "        insights = predictor.get_customer_insights(customer_id)\n",
        "        if insights:\n",
        "            result += \"\\nðŸ“Š CUSTOMER INSIGHTS:\\n\"\n",
        "            result += \"-\" * 30 + \"\\n\"\n",
        "            result += f\"   Age: {insights['age']}\\n\"\n",
        "            result += f\"   Income Band: {insights['income_band']}\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        return f\"âŒ File error: {e}\"\n",
        "    except ValueError as e:\n",
        "        return f\"âŒ Customer error: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Prediction error: {str(e)}\"\n",
        "\n",
        "def batch_predict(customer_ids):\n",
        "    \"\"\"Batch prediction function for multiple customers\"\"\"\n",
        "    print(f\"\\nðŸ” BATCH PREDICTION FOR {len(customer_ids)} CUSTOMERS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        predictor = LightGBMPredictor('lightgbm_channel_ranker.pkl')\n",
        "        predictor.load_customer_data('features_with_channel_labels.csv')\n",
        "        predictor.load_processed_data('processed_data.csv')\n",
        "        predictions = predictor.predict_multiple_customers(customer_ids)\n",
        "        return predictions\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Batch prediction error: {e}\")\n",
        "        return {}\n",
        "\n",
        "def interactive_prediction():\n",
        "    \"\"\"Interactive interface for predicting channel preferences\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ” LIGHTGBM CHANNEL PREFERENCE PREDICTOR\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ“Š Using features_with_channel_labels.csv for predictions\")\n",
        "    print(\"ðŸ“Š Using processed_data.csv for age and income band information\")\n",
        "    print(\"ðŸ“ˆ Scores are normalized to 0-1 range using softmax\")\n",
        "\n",
        "    try:\n",
        "        predictor = LightGBMPredictor('lightgbm_channel_ranker.pkl')\n",
        "        predictor.load_customer_data('features_with_channel_labels.csv')\n",
        "        predictor.load_processed_data('processed_data.csv')\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Initialization failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Show sample customer IDs\n",
        "    sample_customers = predictor.df['Customer_id'].head(10).tolist()\n",
        "    print(f\"\\nðŸ“‹ Sample Customer IDs (first 10):\")\n",
        "    for i, cust_id in enumerate(sample_customers, 1):\n",
        "        print(f\"   {i:2d}. {cust_id}\")\n",
        "\n",
        "    while True:\n",
        "        print(\"\\n\" + \"-\" * 60)\n",
        "        print(\"Prediction Options:\")\n",
        "        print(\"1. ðŸ” Predict for a specific customer\")\n",
        "        print(\"2. ðŸ“Š Predict for multiple customers\")\n",
        "        print(\"3. ðŸŒ Predict for all customers\")\n",
        "        print(\"4. ðŸ’¾ Save current predictions to CSV\")\n",
        "        print(\"5. âŒ Exit\")\n",
        "\n",
        "        choice = input(\"\\nEnter your choice (1-5): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            customer_id = input(\"Enter Customer ID: \").strip()\n",
        "            if customer_id:\n",
        "                result = predict_single_customer(customer_id)\n",
        "                print(result)\n",
        "\n",
        "        elif choice == '2':\n",
        "            customer_ids_input = input(\"Enter Customer IDs (comma-separated): \").strip()\n",
        "            customer_ids = [cid.strip() for cid in customer_ids_input.split(',')]\n",
        "\n",
        "            if customer_ids:\n",
        "                predictions = batch_predict(customer_ids)\n",
        "                if predictions:\n",
        "                    print(predictor.format_predictions(predictions))\n",
        "\n",
        "        elif choice == '3':\n",
        "            confirm = input(\"Predict for ALL customers? This may take time. (y/n): \").strip().lower()\n",
        "            if confirm in ['y', 'yes']:\n",
        "                predictions = predictor.predict_multiple_customers()\n",
        "                print(predictor.format_predictions(predictions))\n",
        "\n",
        "        elif choice == '4':\n",
        "            try:\n",
        "                output_path = input(\"Enter output file name (default: lightgbm_predictions.csv): \").strip()\n",
        "                if not output_path:\n",
        "                    output_path = 'lightgbm_predictions.csv'\n",
        "\n",
        "                # Get predictions for all customers\n",
        "                predictions = predictor.predict_multiple_customers()\n",
        "                saved_file = predictor.save_predictions_to_csv(predictions, output_path)\n",
        "                if saved_file is not None:\n",
        "                    print(f\"âœ… Successfully saved {len(saved_file)} predictions to {output_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error saving file: {e}\")\n",
        "\n",
        "        elif choice == '5':\n",
        "            print(\"ðŸ‘‹ Thank you for using LightGBM Predictor!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"âŒ Invalid choice. Please try again.\")\n",
        "\n",
        "# ðŸš€ MAIN EXECUTION\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸš€ LIGHTGBM CHANNEL PREFERENCE PREDICTION SYSTEM\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ðŸ“Š Using features_with_channel_labels.csv for predictions\")\n",
        "    print(\"ðŸ“Š Using processed_data.csv for age and income band information\")\n",
        "    print(\"ðŸ“ˆ Scores are normalized to 0-1 range using softmax\")\n",
        "\n",
        "    # Directly launch interactive interface\n",
        "    interactive_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b_S89XblPqs",
        "outputId": "d940b378-1943-4d5e-ae6e-035c0f5603d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ LIGHTGBM CHANNEL PREFERENCE PREDICTION SYSTEM\n",
            "============================================================\n",
            "ðŸ“Š Using features_with_channel_labels.csv for predictions\n",
            "ðŸ“Š Using processed_data.csv for age and income band information\n",
            "ðŸ“ˆ Scores are normalized to 0-1 range using softmax\n",
            "============================================================\n",
            "ðŸ” LIGHTGBM CHANNEL PREFERENCE PREDICTOR\n",
            "============================================================\n",
            "ðŸ“Š Using features_with_channel_labels.csv for predictions\n",
            "ðŸ“Š Using processed_data.csv for age and income band information\n",
            "ðŸ“ˆ Scores are normalized to 0-1 range using softmax\n",
            "ðŸ”§ Initializing LightGBM Predictor...\n",
            "âœ… LightGBM model loaded successfully!\n",
            "   Channels: ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
            "ðŸ“ Loading customer data from features_with_channel_labels.csv...\n",
            "âœ… Loaded 100000 customers\n",
            "âœ… Columns in data: 56\n",
            "ðŸŽ¯ Predicting for all 100000 customers\n",
            "ðŸ“ Loading processed data from processed_data.csv...\n",
            "âœ… Loaded 100000 customers from processed data\n",
            "\n",
            "ðŸ“‹ Sample Customer IDs (first 10):\n",
            "    1. SCB843421788\n",
            "    2. SCB998027725\n",
            "    3. SCB871158951\n",
            "    4. SCB938686930\n",
            "    5. SCB843983697\n",
            "    6. SCB946928259\n",
            "    7. SCB900671743\n",
            "    8. SCB882059819\n",
            "    9. SCB892183816\n",
            "   10. SCB831408653\n",
            "\n",
            "------------------------------------------------------------\n",
            "Prediction Options:\n",
            "1. ðŸ” Predict for a specific customer\n",
            "2. ðŸ“Š Predict for multiple customers\n",
            "3. ðŸŒ Predict for all customers\n",
            "4. ðŸ’¾ Save current predictions to CSV\n",
            "5. âŒ Exit\n",
            "\n",
            "Enter your choice (1-5): 1\n",
            "Enter Customer ID: SCB892183816\n",
            "\n",
            "ðŸ” PREDICTION FOR: SCB892183816\n",
            "==================================================\n",
            "ðŸ”§ Initializing LightGBM Predictor...\n",
            "âœ… LightGBM model loaded successfully!\n",
            "   Channels: ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
            "ðŸ“ Loading customer data from features_with_channel_labels.csv...\n",
            "âœ… Loaded 100000 customers\n",
            "âœ… Columns in data: 56\n",
            "ðŸŽ¯ Predicting for customer: SCB892183816\n",
            "ðŸ“ Loading processed data from processed_data.csv...\n",
            "âœ… Loaded 100000 customers from processed data\n",
            "   ðŸŽ¯ Making predictions...\n",
            "   ðŸ”§ Preparing 46 features...\n",
            "   ðŸ”§ Encoding 1 categorical columns...\n",
            "   âœ… Predictions completed for SCB892183816\n",
            "\n",
            "ðŸŽ¯ CHANNEL PREFERENCE PREDICTION\n",
            "==================================================\n",
            "Customer: SCB892183816\n",
            "==================================================\n",
            " 1. SMS          Score:  0.9908 â˜…\n",
            " 2. Email        Score:  0.0056 â˜…â˜…\n",
            " 3. IVR          Score:  0.0023 â˜…â˜…â˜…\n",
            " 4. Field_Agent  Score:  0.0011 â˜…â˜…â˜…â˜…\n",
            " 5. Call         Score:  0.0001 â˜…â˜…â˜…â˜…â˜…\n",
            " 6. WhatsApp     Score:  0.0000 â˜…â˜…â˜…â˜…â˜…\n",
            "\n",
            "ðŸ’¡ Confidence: HIGH (Clear preference)\n",
            "ðŸš€ RECOMMENDATION: Start with SMS\n",
            "\n",
            "ðŸ“Š CUSTOMER INSIGHTS:\n",
            "------------------------------\n",
            "   Age: 30\n",
            "   Income Band: 1\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "Prediction Options:\n",
            "1. ðŸ” Predict for a specific customer\n",
            "2. ðŸ“Š Predict for multiple customers\n",
            "3. ðŸŒ Predict for all customers\n",
            "4. ðŸ’¾ Save current predictions to CSV\n",
            "5. âŒ Exit\n",
            "\n",
            "Enter your choice (1-5): 5\n",
            "ðŸ‘‹ Thank you for using LightGBM Predictor!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ChannelRankingDataPreparator:\n",
        "    \"\"\"Prepare data for Random Forest Ranker training\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        self.label_encoders = {}\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def prepare_ranking_data(self, df):\n",
        "        \"\"\"Convert preference data to ranking format\"\"\"\n",
        "        print(\"Preparing ranking format...\")\n",
        "\n",
        "        # Get feature columns (exclude label columns and Customer_id)\n",
        "        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']\n",
        "        exclude_cols.extend([col for col in df.columns if 'Prefers_' in col])\n",
        "        exclude_cols.extend(['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs'])\n",
        "\n",
        "        self.feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Encode categorical columns\n",
        "        X = df[self.feature_cols].copy()\n",
        "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        if categorical_cols:\n",
        "            print(f\"Encoding {len(categorical_cols)} categorical columns...\")\n",
        "            for col in categorical_cols:\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        # Create ranking dataset\n",
        "        ranking_data = []\n",
        "        group_sizes = []\n",
        "\n",
        "        print(f\"Processing {len(df)} customers...\")\n",
        "\n",
        "        for idx, (_, row) in enumerate(df.iterrows()):\n",
        "            if (idx + 1) % 10000 == 0:\n",
        "                print(f\"Processed {idx + 1:,} customers\")\n",
        "\n",
        "            customer_id = row['Customer_id']\n",
        "            preference_order = row['Channel_Preference_Order'].split(',')\n",
        "\n",
        "            # Get customer features\n",
        "            customer_features = X.iloc[idx].values\n",
        "\n",
        "            # Create one sample per channel\n",
        "            for rank, channel in enumerate(preference_order):\n",
        "                if channel in self.channels:\n",
        "                    # Channel one-hot encoding\n",
        "                    channel_features = np.zeros(len(self.channels))\n",
        "                    channel_idx = self.channels.index(channel)\n",
        "                    channel_features[channel_idx] = 1\n",
        "\n",
        "                    # Combine customer and channel features\n",
        "                    combined_features = np.concatenate([customer_features, channel_features])\n",
        "\n",
        "                    # Relevance score as integer\n",
        "                    relevance = len(self.channels) - rank\n",
        "\n",
        "                    ranking_data.append({\n",
        "                        'customer_id': customer_id,\n",
        "                        'channel': channel,\n",
        "                        'features': combined_features,\n",
        "                        'relevance': relevance,\n",
        "                        'group_id': idx\n",
        "                    })\n",
        "\n",
        "            group_sizes.append(len(self.channels))\n",
        "\n",
        "        # Convert to arrays\n",
        "        X_ranking = np.array([item['features'] for item in ranking_data])\n",
        "        y_ranking = np.array([item['relevance'] for item in ranking_data])\n",
        "        groups = np.array(group_sizes)\n",
        "\n",
        "        print(f\"\\nRanking dataset created:\")\n",
        "        print(f\"Total samples: {len(X_ranking):,}\")\n",
        "        print(f\"Total customers (groups): {len(groups):,}\")\n",
        "        print(f\"Features per sample: {X_ranking.shape[1]}\")\n",
        "        print(f\"Group sizes (samples per customer): {groups[0]} (all should be {len(self.channels)})\")\n",
        "        print(f\"Relevance score range: {y_ranking.min()} to {y_ranking.max()}\")\n",
        "\n",
        "        return X_ranking, y_ranking, groups\n",
        "\n",
        "def train_random_forest_ranker(X, y, groups, test_size=0.2, random_state=42):\n",
        "    \"\"\"Train Random Forest model for ranking\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING RANDOM FOREST RANKER MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Calculate split points for groups\n",
        "    n_train_groups = int(len(groups) * (1 - test_size))\n",
        "    train_samples = sum(groups[:n_train_groups])\n",
        "\n",
        "    # Split data\n",
        "    X_train = X[:train_samples]\n",
        "    y_train = y[:train_samples]\n",
        "    groups_train = groups[:n_train_groups]\n",
        "\n",
        "    X_test = X[train_samples:]\n",
        "    y_test = y[train_samples:]\n",
        "    groups_test = groups[n_train_groups:]\n",
        "\n",
        "    print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "    print(f\"Label vector shape: {y_train.shape}\")\n",
        "    print(f\"Number of groups: {len(groups_train):,}\")\n",
        "    print(f\"\\nTrain set: {len(X_train):,} samples from {len(groups_train):,} customers\")\n",
        "    print(f\"Test set: {len(X_test):,} samples from {len(groups_test):,} customers\")\n",
        "\n",
        "    # Random Forest parameters\n",
        "    rf_params = {\n",
        "        'n_estimators': 100,\n",
        "        'max_depth': 10,\n",
        "        'min_samples_split': 5,\n",
        "        'min_samples_leaf': 2,\n",
        "        'max_features': 'sqrt',\n",
        "        'bootstrap': True,\n",
        "        'random_state': random_state,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': 0  # Suppress training output\n",
        "    }\n",
        "\n",
        "    # Initialize and train Random Forest\n",
        "    print(\"\\nTraining Random Forest...\")\n",
        "    model = RandomForestRegressor(**rf_params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Model training completed successfully!\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nMaking predictions...\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate NDCG\n",
        "    print(\"Calculating NDCG score...\")\n",
        "    # Reshape predictions for NDCG calculation\n",
        "    y_test_reshaped = []\n",
        "    y_pred_reshaped = []\n",
        "\n",
        "    start_idx = 0\n",
        "    for group_size in groups_test:\n",
        "        end_idx = start_idx + group_size\n",
        "        y_test_reshaped.append(y_test[start_idx:end_idx])\n",
        "        y_pred_reshaped.append(y_pred[start_idx:end_idx])\n",
        "        start_idx = end_idx\n",
        "\n",
        "    ndcg = ndcg_score(y_test_reshaped, y_pred_reshaped)\n",
        "    print(f\"NDCG Score: {ndcg:.4f}\")\n",
        "\n",
        "    return model, ndcg\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"PREPARING CHANNEL RANKING DATA FOR RANDOM FOREST RANKER\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading features_with_channel_labels.csv...\")\n",
        "    try:\n",
        "        df = pd.read_csv('features_with_channel_labels.csv')\n",
        "        print(f\"Loaded data shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: features_with_channel_labels.csv not found!\")\n",
        "        print(\"Please run the labeling pipeline first.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Prepare data\n",
        "    preparator = ChannelRankingDataPreparator()\n",
        "    X, y, groups = preparator.prepare_ranking_data(df)\n",
        "\n",
        "    # Train model\n",
        "    model, ndcg_score = train_random_forest_ranker(X, y, groups)\n",
        "\n",
        "    # Save model only (no preparator)\n",
        "    model_filename = 'random_forest_channel_ranker.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"\\nModel saved as '{model_filename}'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Feature columns used: {X.shape[1]}\")\n",
        "    print(f\"Model saved as: {model_filename}\")\n",
        "    print(f\"Total customers processed: {len(df):,}\")\n",
        "    print(f\"Final NDCG Score: {ndcg_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO1XrXX9anjr",
        "outputId": "d99c10d9-e810-4beb-a45a-46a8a379c3aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "PREPARING CHANNEL RANKING DATA FOR RANDOM FOREST RANKER\n",
            "============================================================\n",
            "Loading features_with_channel_labels.csv...\n",
            "Loaded data shape: (100000, 56)\n",
            "Preparing ranking format...\n",
            "Encoding 1 categorical columns...\n",
            "Processing 100000 customers...\n",
            "Processed 10,000 customers\n",
            "Processed 20,000 customers\n",
            "Processed 30,000 customers\n",
            "Processed 40,000 customers\n",
            "Processed 50,000 customers\n",
            "Processed 60,000 customers\n",
            "Processed 70,000 customers\n",
            "Processed 80,000 customers\n",
            "Processed 90,000 customers\n",
            "Processed 100,000 customers\n",
            "\n",
            "Ranking dataset created:\n",
            "Total samples: 600,000\n",
            "Total customers (groups): 100,000\n",
            "Features per sample: 52\n",
            "Group sizes (samples per customer): 6 (all should be 6)\n",
            "Relevance score range: 1 to 6\n",
            "\n",
            "============================================================\n",
            "TRAINING RANDOM FOREST RANKER MODEL\n",
            "============================================================\n",
            "Feature matrix shape: (480000, 52)\n",
            "Label vector shape: (480000,)\n",
            "Number of groups: 80,000\n",
            "\n",
            "Train set: 480,000 samples from 80,000 customers\n",
            "Test set: 120,000 samples from 20,000 customers\n",
            "\n",
            "Training Random Forest...\n",
            "Model training completed successfully!\n",
            "\n",
            "Making predictions...\n",
            "Calculating NDCG score...\n",
            "NDCG Score: 0.9977\n",
            "\n",
            "Model saved as 'random_forest_channel_ranker.pkl'\n",
            "\n",
            "============================================================\n",
            "MODEL TRAINING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Feature columns used: 52\n",
            "Model saved as: random_forest_channel_ranker.pkl\n",
            "Total customers processed: 100,000\n",
            "Final NDCG Score: 0.9977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import ndcg_score\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ModelComparison:\n",
        "    \"\"\"Compare XGBoost, LightGBM, and Random Forest models based on NDCG scores\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all trained models\"\"\"\n",
        "        print(\"ðŸ“ Loading trained models...\")\n",
        "\n",
        "        model_files = {\n",
        "            'XGBoost': 'xgb_channel_ranker.pkl',\n",
        "            'LightGBM': 'lightgbm_channel_ranker.pkl',\n",
        "            'Random Forest': 'random_forest_channel_ranker.pkl'\n",
        "        }\n",
        "\n",
        "        for model_name, file_path in model_files.items():\n",
        "            try:\n",
        "                self.models[model_name] = joblib.load(file_path)\n",
        "                print(f\"âœ… {model_name} loaded successfully\")\n",
        "            except FileNotFoundError:\n",
        "                print(f\"âŒ {model_name} model file not found: {file_path}\")\n",
        "\n",
        "        return len(self.models) > 0\n",
        "\n",
        "    def load_test_data(self, data_path='features_with_channel_labels.csv'):\n",
        "        \"\"\"Load and prepare test data for evaluation\"\"\"\n",
        "        print(\"\\nðŸ“Š Loading test data...\")\n",
        "        self.df = pd.read_csv(data_path)\n",
        "\n",
        "        # Use the same data preparation as during training\n",
        "        exclude_cols = ['Customer_id', 'Channel_Preference_Order', 'Preference_Label', 'Top_Channel']\n",
        "        exclude_cols.extend([col for col in self.df.columns if 'Prefers_' in col])\n",
        "        exclude_cols.extend(['Last_Successful_Agent_ID', 'Best_Contact_Agent_IDs'])\n",
        "        self.feature_cols = [col for col in self.df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Prepare ranking format data\n",
        "        X, y, groups = self.prepare_ranking_data(self.df)\n",
        "\n",
        "        # Split for evaluation (use last 20% as test set)\n",
        "        n_test_groups = int(len(groups) * 0.2)\n",
        "        test_samples = sum(groups[-n_test_groups:])\n",
        "\n",
        "        self.X_test = X[-test_samples:]\n",
        "        self.y_test = y[-test_samples:]\n",
        "        self.groups_test = groups[-n_test_groups:]\n",
        "\n",
        "        print(f\"ðŸ“ˆ Test set: {len(self.X_test):,} samples from {len(self.groups_test):,} customers\")\n",
        "        return True\n",
        "\n",
        "    def prepare_ranking_data(self, df):\n",
        "        \"\"\"Prepare data in ranking format (same as training)\"\"\"\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "        # Encode categorical features\n",
        "        X = df[self.feature_cols].copy()\n",
        "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "        label_encoders = {}\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            X[col] = le.fit_transform(X[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "\n",
        "        # Create ranking dataset\n",
        "        channels = ['Call', 'SMS', 'WhatsApp', 'Email', 'IVR', 'Field_Agent']\n",
        "        ranking_data = []\n",
        "        group_sizes = []\n",
        "\n",
        "        for idx, (_, row) in enumerate(df.iterrows()):\n",
        "            preference_order = row['Channel_Preference_Order'].split(',')\n",
        "            customer_features = X.iloc[idx].values\n",
        "\n",
        "            for rank, channel in enumerate(preference_order):\n",
        "                if channel in channels:\n",
        "                    channel_features = np.zeros(len(channels))\n",
        "                    channel_idx = channels.index(channel)\n",
        "                    channel_features[channel_idx] = 1\n",
        "\n",
        "                    combined_features = np.concatenate([customer_features, channel_features])\n",
        "                    relevance = len(channels) - rank\n",
        "\n",
        "                    ranking_data.append({\n",
        "                        'features': combined_features,\n",
        "                        'relevance': relevance,\n",
        "                        'group_id': idx\n",
        "                    })\n",
        "\n",
        "            group_sizes.append(len(channels))\n",
        "\n",
        "        X_ranking = np.array([item['features'] for item in ranking_data])\n",
        "        y_ranking = np.array([item['relevance'] for item in ranking_data])\n",
        "        groups = np.array(group_sizes)\n",
        "\n",
        "        return X_ranking, y_ranking, groups\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        \"\"\"Evaluate all models on test data\"\"\"\n",
        "        print(\"\\nðŸ“ˆ Evaluating models on test data...\")\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\nðŸ” Evaluating {model_name}...\")\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(self.X_test)\n",
        "\n",
        "            # Calculate NDCG\n",
        "            ndcg = self.calculate_ndcg(self.y_test, y_pred, self.groups_test)\n",
        "\n",
        "            self.results[model_name] = {\n",
        "                'ndcg': ndcg,\n",
        "                'predictions': y_pred\n",
        "            }\n",
        "\n",
        "            print(f\"   âœ… NDCG: {ndcg:.4f}\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def calculate_ndcg(self, y_true, y_pred, groups):\n",
        "        \"\"\"Calculate NDCG score for ranking evaluation\"\"\"\n",
        "        y_true_reshaped = []\n",
        "        y_pred_reshaped = []\n",
        "\n",
        "        start_idx = 0\n",
        "        for group_size in groups:\n",
        "            end_idx = start_idx + group_size\n",
        "            y_true_reshaped.append(y_true[start_idx:end_idx])\n",
        "            y_pred_reshaped.append(y_pred[start_idx:end_idx])\n",
        "            start_idx = end_idx\n",
        "\n",
        "        return ndcg_score(y_true_reshaped, y_pred_reshaped)\n",
        "\n",
        "    def generate_comparison_report(self):\n",
        "        \"\"\"Generate model comparison report sorted by NDCG score\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ† MODEL COMPARISON RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Create comparison table\n",
        "        comparison_data = []\n",
        "        for model_name, results in self.results.items():\n",
        "            comparison_data.append({\n",
        "                'Model': model_name,\n",
        "                'NDCG Score': results['ndcg']\n",
        "            })\n",
        "\n",
        "        # Sort by NDCG score (highest first)\n",
        "        df_comparison = pd.DataFrame(comparison_data)\n",
        "        df_comparison = df_comparison.sort_values('NDCG Score', ascending=False)\n",
        "        df_comparison['Rank'] = range(1, len(df_comparison) + 1)\n",
        "\n",
        "        print(\"\\n\" + df_comparison.to_string(index=False))\n",
        "\n",
        "        # Winner announcement\n",
        "        best_model = df_comparison.iloc[0]\n",
        "        print(f\"\\nðŸŽ¯ BEST PERFORMING MODEL: {best_model['Model']}\")\n",
        "        print(f\"   â€¢ NDCG Score: {best_model['NDCG Score']:.4f}\")\n",
        "        print(f\"   â€¢ Rank: #{best_model['Rank']}\")\n",
        "\n",
        "        # Performance gaps\n",
        "        if len(df_comparison) > 1:\n",
        "            best_score = best_model['NDCG Score']\n",
        "            second_best = df_comparison.iloc[1]['NDCG Score']\n",
        "            gap = best_score - second_best\n",
        "            print(f\"   â€¢ Performance gap: {gap:.4f} over second best\")\n",
        "\n",
        "        return df_comparison\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ¤– MODEL COMPARISON: XGBoost vs LightGBM vs Random Forest\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize comparison\n",
        "    comparator = ModelComparison()\n",
        "\n",
        "    # Load models\n",
        "    if not comparator.load_models():\n",
        "        print(\"âŒ Failed to load models. Please ensure all model files exist.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Load test data\n",
        "    if not comparator.load_test_data():\n",
        "        print(\"âŒ Failed to load test data.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Evaluate models\n",
        "    comparator.evaluate_models()\n",
        "\n",
        "    # Generate comparison report\n",
        "    results_df = comparator.generate_comparison_report()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ… MODEL COMPARISON COMPLETED!\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EH_HXz7fScT",
        "outputId": "f1657dbb-eb27-4529-cc55-5677dede5e6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ¤– MODEL COMPARISON: XGBoost vs LightGBM vs Random Forest\n",
            "============================================================\n",
            "ðŸ“ Loading trained models...\n",
            "âœ… XGBoost loaded successfully\n",
            "âœ… LightGBM loaded successfully\n",
            "âœ… Random Forest loaded successfully\n",
            "\n",
            "ðŸ“Š Loading test data...\n",
            "ðŸ“ˆ Test set: 120,000 samples from 20,000 customers\n",
            "\n",
            "ðŸ“ˆ Evaluating models on test data...\n",
            "\n",
            "ðŸ” Evaluating XGBoost...\n",
            "   âœ… NDCG: 0.9991\n",
            "\n",
            "ðŸ” Evaluating LightGBM...\n",
            "   âœ… NDCG: 0.9998\n",
            "\n",
            "ðŸ” Evaluating Random Forest...\n",
            "   âœ… NDCG: 0.9977\n",
            "\n",
            "============================================================\n",
            "ðŸ† MODEL COMPARISON RESULTS\n",
            "============================================================\n",
            "\n",
            "        Model  NDCG Score  Rank\n",
            "     LightGBM    0.999807     1\n",
            "      XGBoost    0.999092     2\n",
            "Random Forest    0.997741     3\n",
            "\n",
            "ðŸŽ¯ BEST PERFORMING MODEL: LightGBM\n",
            "   â€¢ NDCG Score: 0.9998\n",
            "   â€¢ Rank: #1\n",
            "   â€¢ Performance gap: 0.0007 over second best\n",
            "\n",
            "============================================================\n",
            "âœ… MODEL COMPARISON COMPLETED!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "495-NvSIaoKe"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}